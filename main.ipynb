{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in /Users/pratham28/miniconda3/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: requests~=2.32.0 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from arxiv) (2.32.4)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: sgmllib3k in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (2023.7.22)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (1.26.15)\n",
      "Requirement already satisfied: markdownify in /Users/pratham28/miniconda3/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from markdownify) (4.12.2)\n",
      "Requirement already satisfied: six<2,>=1.15 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from markdownify) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from beautifulsoup4<5,>=4.9->markdownify) (2.5)\n",
      "Requirement already satisfied: pymupdf in /Users/pratham28/miniconda3/lib/python3.10/site-packages (1.26.3)\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m929.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pypdfium2>=4.18.0\n",
      "  Downloading pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from pdfplumber) (9.4.0)\n",
      "Collecting pdfminer.six==20250506\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=36.0.0 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from pdfminer.six==20250506->pdfplumber) (39.0.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from pdfminer.six==20250506->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n",
      "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.1\n",
      "Collecting reportlab\n",
      "  Downloading reportlab-4.4.2-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from reportlab) (2.0.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/pratham28/miniconda3/lib/python3.10/site-packages (from reportlab) (9.4.0)\n",
      "Installing collected packages: reportlab\n",
      "Successfully installed reportlab-4.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv\n",
    "!pip install markdownify\n",
    "!pip install pymupdf\n",
    "!pip install pdfplumber\n",
    "!pip install reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import networkx as nx\n",
    "\n",
    "# -----------------------------\n",
    "# Local LLM query function (Ollama)\n",
    "# -----------------------------\n",
    "def query_local_llm(prompt, model=\"llama3\"):\n",
    "    command = f'echo \"{prompt}\" | ollama run {model}'\n",
    "    result = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    return result.stdout.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a very simple Python program that prints \"Hello, World!\" to the screen:\\n```\\nprint(\"Hello, World!\")\\n```\\nThis is often referred to as the \"Hello World\" program because it\\'s a classic example of a first program in many programming languages.\\n\\nIf you want to run this code, just save it to a file with a `.py` extension (e.g. `hello.py`) and then run it using Python (e.g. `python hello.py`). The output will be:\\n```\\nHello, World!\\n```\\nLet me know if you have any questions or if you\\'d like to see more examples!\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Give me a simple python code\"\"\"\n",
    "query_local_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "def get_arxiv_papers(query, max_results=5):\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    client = arxiv.Client()\n",
    "    papers = []\n",
    "    for result in client.results(search):\n",
    "        papers.append({\n",
    "            \"title\": result.title,\n",
    "            \"abstract\": result.summary,\n",
    "            \"url\": result.pdf_url\n",
    "        })\n",
    "    return papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Retrieving papers on: Topological Machine Learning\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'How to Organize your Deep Reinforcement Learning Agents: The Importance of Communication Topology',\n",
       "  'abstract': 'In this empirical paper, we investigate how learning agents can be arranged\\nin more efficient communication topologies for improved learning. This is an\\nimportant problem because a common technique to improve speed and robustness of\\nlearning in deep reinforcement learning and many other machine learning\\nalgorithms is to run multiple learning agents in parallel. The standard\\ncommunication architecture typically involves all agents intermittently\\ncommunicating with each other (fully connected topology) or with a centralized\\nserver (star topology). Unfortunately, optimizing the topology of communication\\nover the space of all possible graphs is a hard problem, so we borrow results\\nfrom the networked optimization and collective intelligence literatures which\\nsuggest that certain families of network topologies can lead to strong\\nimprovements over fully-connected networks. We start by introducing alternative\\nnetwork topologies to DRL benchmark tasks under the Evolution Strategies\\nparadigm which we call Network Evolution Strategies. We explore the relative\\nperformance of the four main graph families and observe that one such family\\n(Erdos-Renyi random graphs) empirically outperforms all other families,\\nincluding the de facto fully-connected communication topologies. Additionally,\\nthe use of alternative network topologies has a multiplicative performance\\neffect: we observe that when 1000 learning agents are arranged in a carefully\\ndesigned communication topology, they can compete with 3000 agents arranged in\\nthe de facto fully-connected topology. Overall, our work suggests that\\ndistributed machine learning algorithms would learn more efficiently if the\\ncommunication topology between learning agents was optimized.',\n",
       "  'url': 'http://arxiv.org/pdf/1811.12556v2'},\n",
       " {'title': 'Persformer: A Transformer Architecture for Topological Machine Learning',\n",
       "  'abstract': 'One of the main challenges of Topological Data Analysis (TDA) is to extract\\nfeatures from persistent diagrams directly usable by machine learning\\nalgorithms. Indeed, persistence diagrams are intrinsically (multi-)sets of\\npoints in $\\\\mathbb{R}^2$ and cannot be seen in a straightforward manner as\\nvectors. In this article, we introduce $\\\\texttt{Persformer}$, the first\\nTransformer neural network architecture that accepts persistence diagrams as\\ninput. The $\\\\texttt{Persformer}$ architecture significantly outperforms\\nprevious topological neural network architectures on classical synthetic and\\ngraph benchmark datasets. Moreover, it satisfies a universal approximation\\ntheorem. This allows us to introduce the first interpretability method for\\ntopological machine learning, which we explore in two examples.',\n",
       "  'url': 'http://arxiv.org/pdf/2112.15210v2'},\n",
       " {'title': 'Lale: Consistent Automated Machine Learning',\n",
       "  'abstract': 'Automated machine learning makes it easier for data scientists to develop\\npipelines by searching over possible choices for hyperparameters, algorithms,\\nand even pipeline topologies. Unfortunately, the syntax for automated machine\\nlearning tools is inconsistent with manual machine learning, with each other,\\nand with error checks. Furthermore, few tools support advanced features such as\\ntopology search or higher-order operators. This paper introduces Lale, a\\nlibrary of high-level Python interfaces that simplifies and unifies automated\\nmachine learning in a consistent way.',\n",
       "  'url': 'http://arxiv.org/pdf/2007.01977v1'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_topic = \"\"\"Topological Machine Learning\"\"\"\n",
    "print(f\"🔎 Retrieving papers on: {query_topic}\\n\")\n",
    "papers = get_arxiv_papers(query_topic, max_results=3)\n",
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9257"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz\n",
    "pdf_path = \"./tmp/paper_2103.08134v1.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "full_text = \"\"\n",
    "for page in doc:\n",
    "    full_text += page.get_text()\n",
    "doc.close()\n",
    "full_text\n",
    "full_text_arr = full_text.split()\n",
    "len(full_text_arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Could not find 'Introduction' heading.\n",
      "⚠️ No section headings found.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "ROMANS = ['I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X',\n",
    "          'XI', 'XII', 'XIII', 'XIV', 'XV', 'XVI', 'XVII', 'XVIII', 'XIX', 'XX']\n",
    "\n",
    "# === Step 1: Extract all lines with font sizes ===\n",
    "def extract_lines_with_fonts(pdf_path):\n",
    "    lines = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            words = page.extract_words(extra_attrs=['size'])\n",
    "            grouped = defaultdict(list)\n",
    "            for w in words:\n",
    "                grouped[round(w['top'], 1)].append(w)\n",
    "\n",
    "            for line_top in sorted(grouped):\n",
    "                line_words = grouped[line_top]\n",
    "                text = ' '.join(w['text'] for w in line_words)\n",
    "                sizes = [round(w['size'], 2) for w in line_words]\n",
    "                common_size = max(set(sizes), key=sizes.count)  # most frequent size in the line\n",
    "                lines.append({\n",
    "                    \"page\": page_num,\n",
    "                    \"text\": text.strip(),\n",
    "                    \"font_size\": common_size\n",
    "                })\n",
    "    return lines\n",
    "\n",
    "# === Step 2: Detect 'Introduction' and extract prefix + font size ===\n",
    "def detect_intro_prefix_and_font(lines):\n",
    "    for line in lines:\n",
    "        match = re.match(r\"^(\\S+)?\\.?\\s*Introduction\", line[\"text\"], re.IGNORECASE)\n",
    "        if match:\n",
    "            print(line)\n",
    "            prefix = match.group(1).upper()\n",
    "            print(f\"✅ Found 'Introduction': {line['text']} (Page {line['page']}) Font: {line['font_size']}\")\n",
    "            return prefix, line['font_size'], line['text'], line['page']\n",
    "    return None, None, None, None\n",
    "\n",
    "# === Step 3: Classify prefix type ===\n",
    "def classify_prefix_type(prefix):\n",
    "    if not prefix:\n",
    "        return None\n",
    "    prefix = prefix.strip(\".\").upper()\n",
    "    if prefix.isdigit():\n",
    "        return \"numeric\"\n",
    "    elif prefix in ROMANS:\n",
    "        return \"roman\"\n",
    "    elif re.match(r\"^[A-Z]$\", prefix):\n",
    "        return \"alpha\"\n",
    "    return None\n",
    "\n",
    "# === Step 4: Strict matching with same prefix type and font ===\n",
    "def extract_strict_headings(lines, prefix_type, intro_prefix, intro_font_size):\n",
    "    headings = []\n",
    "    intro_prefix = intro_prefix.strip(\".\").upper()\n",
    "    font_tol = 0.4  # allow for small float difference in font size\n",
    "\n",
    "    if prefix_type == \"numeric\":\n",
    "        current = int(intro_prefix)\n",
    "        next_expected = current + 1\n",
    "        while True:\n",
    "            pattern = re.compile(rf\"^{next_expected}\\.?\\s+[A-Za-z].+\")\n",
    "            found = False\n",
    "            for line in lines:\n",
    "                if pattern.match(line[\"text\"]) and abs(line[\"font_size\"] - intro_font_size) <= font_tol:\n",
    "                    headings.append({\"page\": line[\"page\"], \"heading\": line[\"text\"]})\n",
    "                    next_expected += 1\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                break\n",
    "\n",
    "    elif prefix_type == \"roman\":\n",
    "        current_idx = ROMANS.index(intro_prefix)\n",
    "        for next_roman in ROMANS[current_idx + 1:]:\n",
    "            pattern = re.compile(rf\"^{next_roman}\\.?\\s+[A-Za-z].+\")\n",
    "            for line in lines:\n",
    "                if pattern.match(line[\"text\"]) and abs(line[\"font_size\"] - intro_font_size) <= font_tol:\n",
    "                    headings.append({\"page\": line[\"page\"], \"heading\": line[\"text\"]})\n",
    "                    break\n",
    "\n",
    "    elif prefix_type == \"alpha\":\n",
    "        current = ord(intro_prefix)\n",
    "        for i in range(current + 1, ord('Z') + 1):\n",
    "            next_char = chr(i)\n",
    "            pattern = re.compile(rf\"^{next_char}\\.?\\s+[A-Za-z].+\")\n",
    "            for line in lines:\n",
    "                if pattern.match(line[\"text\"]) and abs(line[\"font_size\"] - intro_font_size) <= font_tol:\n",
    "                    headings.append({\"page\": line[\"page\"], \"heading\": line[\"text\"]})\n",
    "                    break\n",
    "\n",
    "    return headings\n",
    "\n",
    "# === Step 5: Main runner ===\n",
    "def extract_headings_with_fontmatch(pdf_path):\n",
    "    lines = extract_lines_with_fonts(pdf_path)\n",
    "    prefix, font_size, intro_text, intro_page = detect_intro_prefix_and_font(lines)\n",
    "\n",
    "    if not prefix:\n",
    "        print(\"❌ Could not find 'Introduction' heading.\")\n",
    "        return []\n",
    "\n",
    "    prefix_type = classify_prefix_type(prefix)\n",
    "    if not prefix_type:\n",
    "        print(f\"❌ Could not classify prefix '{prefix}'\")\n",
    "        return []\n",
    "\n",
    "    print(f\"📌 Detected prefix type: {prefix_type.upper()} | Font size: {font_size}\")\n",
    "\n",
    "    headings = [{\"page\": intro_page, \"heading\": intro_text}]\n",
    "    headings += extract_strict_headings(lines, prefix_type, prefix, font_size)\n",
    "\n",
    "    return headings\n",
    "\n",
    "# === Example usage ===\n",
    "pdf_path = \"./tmp/paper_1808.08210v3.pdf\"\n",
    "headings = extract_headings_with_fontmatch(pdf_path)\n",
    "\n",
    "# === Print results ===\n",
    "if not headings:\n",
    "    print(\"⚠️ No section headings found.\")\n",
    "else:\n",
    "    print(\"\\n📄 Strict section headings (font & pattern matched):\\n\")\n",
    "    for h in headings:\n",
    "        print(f\"Page {h['page']:>3} | {h['heading']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.95519999999999, 9.962599999999952, 9.962600000000066, 9.962600000000009, 9.962599999999995, 10.0, 6.973799999999983, 4.981300000000033, 9.96259999999998, 6.973800000000011, 6.973799999999997]\n",
      "['SCREENCONTENTIMAGESEGMENTATIONUSINGLEASTABSOLUTEDEVIATION', 'FITTING', 'DepartmentofElectricalandComputerEngineering,PolytechnicSchoolofEngineering,', 'NewYorkUniversity,NY,USA.']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz \n",
    "import pdfplumber\n",
    "from collections import Counter\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.platypus import SimpleDocTemplate, Preformatted\n",
    "\n",
    "\n",
    "\n",
    "def extract_header_fontsize_from_pdf(pdf_path):\n",
    "    font_size_counter = Counter()\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i in range(len(pdf.pages)):\n",
    "            # lines1 = pdf.pages[i].extract_text().split('\\n')\n",
    "            words = pdf.pages[i].extract_words(extra_attrs=['fontname', 'size'])\n",
    "            lines = {}\n",
    "\n",
    "            for word in words:\n",
    "                line_num = word['top']\n",
    "                if line_num not in lines:\n",
    "                    lines[line_num] = []\n",
    "                lines[line_num].append(word)\n",
    "\n",
    "            for line_words in lines.values():\n",
    "                font_size_counter[line_words[0]['size']] += 1\n",
    "\n",
    "    # Find the font sizes that were used more than once\n",
    "    repeated_sizes = [size for size, count in font_size_counter.items() if count > 3]\n",
    "    print(repeated_sizes)\n",
    "    # Return the highest font size among the repeated sizes\n",
    "    if repeated_sizes:\n",
    "        return max(repeated_sizes)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_lines_with_font_size(pdf_path, target_font_size):\n",
    "    lines_with_target_font_size = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i in range(len(pdf.pages)):\n",
    "            words = pdf.pages[i].extract_words(extra_attrs=['fontname', 'size'])\n",
    "            lines = {}\n",
    "\n",
    "            for word in words:\n",
    "                line_num = word['top']\n",
    "                if line_num not in lines:\n",
    "                    lines[line_num] = []\n",
    "                lines[line_num].append(word)\n",
    "\n",
    "            for line_num, line_words in lines.items():\n",
    "                line_font_sizes = [word['size'] for word in line_words]\n",
    "                if target_font_size in line_font_sizes:\n",
    "                    line_text = ' '.join([word['text'] for word in line_words])\n",
    "                    lines_with_target_font_size.append(line_text)\n",
    "\n",
    "    return lines_with_target_font_size\n",
    "\n",
    "pdf1 = \"./tmp/paper_1501.03755v2.pdf\"\n",
    "extracted_font_size = extract_header_fontsize_from_pdf(pdf1)\n",
    "extracted_headers = extract_lines_with_font_size(pdf1,extracted_font_size)\n",
    "print(extracted_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f7/jqgtrh7d1zd6rb0wwb3rzgd00000gn/T/ipykernel_92442/2508475644.py:17: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\n",
      "✅ Done! Your literature review is saved as 'literature_review.md'.\n"
     ]
    }
   ],
   "source": [
    "# Install arxiv module if not installed\n",
    "# pip install arxiv\n",
    "\n",
    "import arxiv\n",
    "\n",
    "def search_papers(topic, max_results=5):\n",
    "    \"\"\"\n",
    "    Search arXiv for papers matching the topic.\n",
    "    \"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=topic,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "        sort_order=arxiv.SortOrder.Descending,\n",
    "    )\n",
    "    results = []\n",
    "    for result in search.results():\n",
    "        result.download_pdf(dirpath=\"./tmp\", filename=f\"paper_{result.get_short_id()}.pdf\")\n",
    "        results.append({\n",
    "            \"title\": result.title,\n",
    "            \"summary\": result.summary,\n",
    "            \"authors\": [a.name for a in result.authors],\n",
    "            \"published\": result.published.strftime('%Y-%m-%d'),\n",
    "            \"link\": result.entry_id,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def summarize_paper_with_cot(paper, model=\"llama3\"):\n",
    "    \"\"\"\n",
    "    Use local LLM (Ollama) to generate summary with chain-of-thought.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful AI research assistant. Given the following paper abstract, please do:\n",
    "\n",
    "1. Summarize the main contribution.\n",
    "2. Describe what methods are used and why they might be effective.\n",
    "3. Explain your reasoning step by step (chain of thought) so it is clear how you interpreted the abstract.\n",
    "4. Suggest possible future directions or improvements.\n",
    "\n",
    "### Abstract:\n",
    "{paper[\"summary\"]}\n",
    "\n",
    "Respond in the following format:\n",
    "\n",
    "Title: <Title>\n",
    "Main Contribution: <...>\n",
    "Methods Used: <...>\n",
    "Chain of Thought: <...>\n",
    "Future Directions: <...>\n",
    "\"\"\"\n",
    "    command = f'echo \"{prompt}\" | ollama run {model}'\n",
    "    result = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    return result.stdout.decode()\n",
    "\n",
    "def generate_report(topic, max_results=3):\n",
    "    \"\"\"\n",
    "    Main function to generate the literature review report.\n",
    "    \"\"\"\n",
    "    papers = search_papers(topic, max_results=max_results)\n",
    "    report = f\"# 📄 Literature Review on: {topic}\\n\\n\"\n",
    "    print(papers)\n",
    "    for idx, paper in enumerate(papers, 1):\n",
    "        report += f\"## 📝 Paper {idx}: {paper['title']}\\n\"\n",
    "        report += f\"- **Authors:** {', '.join(paper['authors'])}\\n\"\n",
    "        report += f\"- **Published:** {paper['published']}\\n\"\n",
    "        report += f\"- **Link:** {paper['link']}\\n\\n\"\n",
    "        print(f\"Summarizing: {paper['title']} ...\")\n",
    "        # summary = summarize_paper_with_cot(paper)\n",
    "        # report += summary + \"\\n\\n---\\n\\n\"\n",
    "\n",
    "    return report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    topic = \"What are the loss functions used in CNN modelling ?\"\n",
    "    final_report = generate_report(topic, max_results=5)\n",
    "\n",
    "    # Save to file\n",
    "    with open(\"literature_review.md\", \"w\") as f:\n",
    "        f.write(final_report)\n",
    "\n",
    "    print(\"\\n✅ Done! Your literature review is saved as 'literature_review.md'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "from typing import List, Dict\n",
    "\n",
    "class PaperLoader:\n",
    "    def __init__(self, api_key: str = None):\n",
    "        self.base_url = \"https://api.semanticscholar.org/graph/v1\"\n",
    "        self.api_key = api_key  # Optional, but increases limits\n",
    "\n",
    "    def fetch_for_query(self, query: str, max_results: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetch relevant papers from Semantic Scholar for a given query.\n",
    "        \"\"\"\n",
    "        headers = {}\n",
    "        if self.api_key:\n",
    "            headers[\"x-api-key\"] = self.api_key\n",
    "        \n",
    "        url = f\"{self.base_url}/paper/search\"\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"limit\": max_results,\n",
    "            \"fields\": \"title,url,abstract,authors,year,openAccessPdf\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        print(response)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        print(data)\n",
    "        results = []\n",
    "        for paper in data.get(\"data\", []):\n",
    "            if paper.get(\"openAccessPdf\") and paper[\"openAccessPdf\"].get(\"url\"):\n",
    "                results.append(self._process_result(paper))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _process_result(self, paper: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a single paper entry from Semantic Scholar.\n",
    "        \"\"\"\n",
    "        pdf_url = paper[\"openAccessPdf\"][\"url\"] if paper.get(\"openAccessPdf\") else None\n",
    "        return {\n",
    "            \"title\": paper[\"title\"],\n",
    "            \"text\": self._extract_text(pdf_url) if pdf_url else \"\",\n",
    "            \"metadata\": {\n",
    "                \"authors\": [a[\"name\"] for a in paper.get(\"authors\", [])],\n",
    "                \"published\": paper.get(\"year\"),\n",
    "                \"abstract\": paper.get(\"abstract\"),\n",
    "                \"url\": paper.get(\"url\")\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _extract_text(self, pdf_url: str) -> str:\n",
    "        \"\"\"\n",
    "        Download and extract text from an open-access PDF.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(pdf_url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            with fitz.open(stream=response.content, filetype=\"pdf\") as doc:\n",
    "                return \"\\n\".join(page.get_text() for page in doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch PDF from {pdf_url}: {e}\")\n",
    "            return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'total': 610199, 'offset': 0, 'next': 3, 'data': [{'paperId': '376aad5815f808c82e518956a70091bf828dbd25', 'url': 'https://www.semanticscholar.org/paper/376aad5815f808c82e518956a70091bf828dbd25', 'title': 'Person Re-identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function', 'year': 2016, 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2016.149?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2016.149, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.\"}, 'authors': [{'authorId': '145067864', 'name': 'De Cheng'}, {'authorId': '144768792', 'name': 'Yihong Gong'}, {'authorId': '3373601', 'name': 'Sanping Zhou'}, {'authorId': '71563118', 'name': 'Jinjun Wang'}, {'authorId': '122737130', 'name': 'N. Zheng'}], 'abstract': None}, {'paperId': '39dbc6e16cff482e5e8a714b44654747d6dde0e0', 'url': 'https://www.semanticscholar.org/paper/39dbc6e16cff482e5e8a714b44654747d6dde0e0', 'title': 'Wind Turbine Blade Icing Prediction Using Focal Loss Function and CNN-Attention-GRU Algorithm', 'year': 2023, 'openAccessPdf': {'url': 'https://www.mdpi.com/1996-1073/16/15/5621/pdf?version=1690425893', 'status': 'GOLD', 'license': 'CCBY', 'disclaimer': 'Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/en16155621?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/en16155621, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2215317192', 'name': 'Cheng Tao'}, {'authorId': '2088189145', 'name': 'Tao Tao'}, {'authorId': '2215352749', 'name': 'Xinjian Bai'}, {'authorId': '91964531', 'name': 'Yongqian Liu'}], 'abstract': 'Blade icing seriously affects wind turbines’ aerodynamic performance and output power. Timely and accurately predicting blade icing status is crucial to improving the economy and safety of wind farms. However, existing blade icing prediction methods cannot effectively solve the problems of unbalanced icing/non-icing data and low prediction accuracy. In order to solve the above problems, this paper proposes a wind turbine blade icing prediction method based on the focal loss function and CNN-Attention-GRU. First, the recursive feature elimination method combined with the physical mechanism of icing is used to extract features highly correlated with blade icing, and a new feature subset is formed through a sliding window algorithm. Then, the focal loss function is utilized to assign more weight to the ice samples with a lower proportion, addressing the significant class imbalance between the ice and non-ice categories. Finally, based on the CNN-Attention-GRU algorithm, a blade icing prediction model is established using continuous 24-h historical data as the input and the icing status of the next 24 h as the output. The model is compared with advanced neural network models. The results show that the proposed method improves the prediction accuracy and F1 score by an average of 6.41% and 4.27%, respectively, demonstrating the accuracy and effectiveness of the proposed method.'}, {'paperId': '42c03d9a955e4b6d8ecdc320b7bf77f7ade5d57e', 'url': 'https://www.semanticscholar.org/paper/42c03d9a955e4b6d8ecdc320b7bf77f7ade5d57e', 'title': 'Automated Lung Sound Classification Using a Hybrid CNN-LSTM Network and Focal Loss Function', 'year': 2022, 'openAccessPdf': {'url': 'https://www.mdpi.com/1424-8220/22/3/1232/pdf?version=1644381413', 'status': 'GOLD', 'license': 'CCBY', 'disclaimer': 'Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8838187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2003481063', 'name': 'G. Petmezas'}, {'authorId': '8224159', 'name': 'Grigorios-Aris Cheimariotis'}, {'authorId': '24683162', 'name': 'Leandros Stefanopoulos'}, {'authorId': '8515942', 'name': 'B. Rocha'}, {'authorId': '144977038', 'name': 'Rui Pedro Paiva'}, {'authorId': '144842935', 'name': 'A. Katsaggelos'}, {'authorId': '144338063', 'name': 'N. Maglaveras'}], 'abstract': 'Respiratory diseases constitute one of the leading causes of death worldwide and directly affect the patient’s quality of life. Early diagnosis and patient monitoring, which conventionally include lung auscultation, are essential for the efficient management of respiratory diseases. Manual lung sound interpretation is a subjective and time-consuming process that requires high medical expertise. The capabilities that deep learning offers could be exploited in order that robust lung sound classification models can be designed. In this paper, we propose a novel hybrid neural model that implements the focal loss (FL) function to deal with training data imbalance. Features initially extracted from short-time Fourier transform (STFT) spectrograms via a convolutional neural network (CNN) are given as input to a long short-term memory (LSTM) network that memorizes the temporal dependencies between data and classifies four types of lung sounds, including normal, crackles, wheezes, and both crackles and wheezes. The model was trained and tested on the ICBHI 2017 Respiratory Sound Database and achieved state-of-the-art results using three different data splitting strategies—namely, sensitivity 47.37%, specificity 82.46%, score 64.92% and accuracy 73.69% for the official 60/40 split, sensitivity 52.78%, specificity 84.26%, score 68.52% and accuracy 76.39% using interpatient 10-fold cross validation, and sensitivity 60.29% and accuracy 74.57% using leave-one-out cross validation.'}]}\n",
      "Failed to fetch PDF from https://www.mdpi.com/1996-1073/16/15/5621/pdf?version=1690425893: 403 Client Error: Forbidden for url: https://www.mdpi.com/1996-1073/16/15/5621/pdf?version=1690425893\n",
      "Failed to fetch PDF from https://www.mdpi.com/1424-8220/22/3/1232/pdf?version=1644381413: 403 Client Error: Forbidden for url: https://www.mdpi.com/1424-8220/22/3/1232/pdf?version=1644381413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PaperLoader()\n",
    "topic =\"loss function cnn\"\n",
    "\n",
    "\n",
    "papers = loader.fetch_for_query(topic)[:3]\n",
    "len(papers)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f7/jqgtrh7d1zd6rb0wwb3rzgd00000gn/T/ipykernel_92442/3425738652.py:12: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  return [self._process_result(r) for r in search.results()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Improving Interpretability and Accuracy in Neuro-Symbolic Rule Extraction Using Class-Specific Sparse Filters',\n",
       "  'text': 'Improving Interpretability and Accuracy in Neuro-Symbolic Rule Extraction\\nUsing Class-Specific Sparse Filters\\nParth Padalkar , Jaeseong Lee , Shiyi Wei and Gopal Gupta\\nThe University of Texas at Dallas\\n{parth.padalkar, jaeseong.lee, swei, gupta}@utdallas.edu\\nAbstract\\nThere has been significant focus on creating neuro-\\nsymbolic models for interpretable image classifica-\\ntion using Convolutional Neural Networks (CNNs).\\nThese methods aim to replace the CNN with\\na neuro-symbolic model consisting of the CNN,\\nwhich is used as a feature extractor, and an in-\\nterpretable rule-set extracted from the CNN it-\\nself. While these approaches provide interpretabil-\\nity through the extracted rule-set, they often com-\\npromise accuracy compared to the original CNN\\nmodel. In this paper, we identify the root cause\\nof this accuracy loss as the post-training binariza-\\ntion of filter activations to extract the rule-set. To\\naddress this, we propose a novel sparsity loss func-\\ntion that enables class-specific filter binarization\\nduring CNN training, thus minimizing information\\nloss when extracting the rule-set. We evaluate sev-\\neral training strategies with our novel sparsity loss,\\nanalyzing their effectiveness and providing guid-\\nance on their appropriate use. Notably, we set a\\nnew benchmark, achieving a 9% improvement in\\naccuracy and a 53% reduction in rule-set size on\\naverage, compared to the previous SOTA, while\\ncoming within 3% of the original CNN’s accuracy.\\nThis highlights the significant potential of inter-\\npretable neuro-symbolic models as viable alterna-\\ntives to black-box CNNs.\\n1\\nIntroduction\\nInterpretability in deep neural models has gained a lot of in-\\nterest in recent years, e.g., [Li et al., 2022; Zhang et al., 2021;\\nR¨auker et al., 2023]. This is well placed, as some applica-\\ntions such as autonomous vehicles [Kanagaraj et al., 2021],\\ndisease diagnosis [Sun et al., 2016], and natural disaster pre-\\nvention [Ko and Kwak, 2012] are very sensitive areas where\\na wrong prediction could be the difference between life and\\ndeath. The above tasks rely heavily on good image classifica-\\ntion models such as Convolutional Neural Networks (CNNs)\\n[LeCun et al., 1989] which are not interpretable. Hence these\\napplications could benefit greatly by using models that bal-\\nance interpretability with decent accuracy.\\nFigure 1: The NeSyFOLD Framework\\nSpecifically, in the context of image classification using\\nCNNs, there has been a lot of effort towards improving their\\ninterpretability by extracting a rule-set from the convolution\\nlayers, that explains the underlying decision-making logic of\\nthe model [Townsend et al., 2021; Townsend et al., 2022;\\nPadalkar et al., 2024a; Padalkar et al., 2024b; Padalkar et al.,\\n2024c]. The rule-set along with the CNN upto the last convo-\\nlutional layer forms the NeSy model, where the final classifi-\\ncation is done by the rule-set. This neuro-symbolic approach,\\nwhere the neural component (CNN) and the symbolic com-\\nponent (rule-set) are used in conjunction for the classification\\ntask, has shown promise in areas such as interpretable Covid-\\n19 and pleural effusion detection from chest X-ray images\\n[Ngan et al., 2022].\\nThe core of these neuro-symbolic methods lies in the bina-\\nrization of the last-layer filter outputs using a threshold calcu-\\nlated post-training. This process converts each training image\\ninto a binarized feature vector derived from the final convolu-\\ntional layer of the CNN. Subsequently, a decision-tree algo-\\nrithm or rule-based machine learning algorithm is employed\\nto generate symbolic rules from these binarized vectors. The\\nclassification is then performed by extracting the outputs of\\nthe last-layer filters, binarizing each filter’s output using the\\narXiv:2501.16677v1  [cs.CV]  28 Jan 2025\\n\\ncalculated threshold, and applying the rule-set for the final\\nclassification. The binarized filter output represents the cor-\\nresponding atom’s/predicate’s truth value in the rule-set. The\\nfinal classification is done based on the rule that fires. Figure\\n1 illustrates the current state-of-the-art (SOTA), NeSyFOLD\\nframework [Padalkar et al., 2024a], for generating a symbolic\\nrule-set from a CNN in the form of a logic program. Notice\\nhow all the filter outputs (f1, f2, ..., fn) are binarized for each\\nimage, creating a binarization table. This table along with the\\ntarget class for each row, is fed to the FOLD-SE-M [Wang and\\nGupta, 2024] rule-based machine algorithm that generates a\\nrule-set in the form of a logic program. The filters appear as\\npredicates in each rule’s body and their binary value (0/1)\\ndetermines their respective predicate’s truth value.\\nThe filter outputs in a CNN are continuous and optimized\\nduring training to support the fully connected layers for final\\nclassification.\\nHowever, in the aforementioned neurosym-\\nbolic methods the fully connected layers are essentially re-\\nplaced with a rule-set for classification. Hence, these contin-\\nuous outputs must be binarized or converted into a symbolic-\\nfriendly form (0/1) to suit the requirements of the rule-\\nextraction algorithm. This post-training binarization leads to\\ninformation loss, as the filters were not originally optimized\\nfor such binarization.\\nOne approach to mitigate this information loss is by train-\\ning the model such that there are very few filters inside the\\nCNN that are responsible for learning the representation of\\neach class. This is known as “learning class-specific sparse\\nfilters” and is typically done using a loss function designed to\\ninduce sparsity [Kasioumis et al., 2021; Zhang et al., 2018;\\nShen et al., 2021]. While this approach reduces the number\\nof filter outputs that require binarization post-training, it only\\npartially addresses the issue, as the fundamental challenge\\nof transitioning from continuous to symbolic binary repre-\\nsentations still persists and hence there remains a significant\\ngap between the accuracy of the original CNN model and the\\nNeSy model.\\nTo address this limitation we propose a novel sparsity\\nloss function which enforces a pre-selected subset of class-\\nspecific filter outputs, to converge towards values close to 1\\nwhile pushing other filter outputs close to 0. Hence, post-\\ntraining, the pseudo-binarized filter outputs for each image\\ncan be directly rounded to create feature vectors, which are\\nmore optimal for post-training rule-extraction.\\nWe show\\nthrough our experiments that using this loss function can lead\\nto significant gains in the accuracy of the NeSy model (9%)\\nas compared to the previous SOTA (NeSyFOLD-EBP) while\\nat the same time drastically reducing the size of the rule-set\\ngenerated by 53%, thus improving interpretability. Thus, the\\naccuracy gap between the trained CNN and the NeSy model\\nis narrowed to an average of just 3%. To summarize, our\\nnovel contributions are as follows:\\n1. We introduce a novel sparsity loss function that can be\\nused during training to learn sparse filters with pseudo-\\nbinarized outputs.\\n2. We present a comprehensive analysis of 5 training strate-\\ngies using the sparsity loss and discuss their merits and\\npitfalls.\\n2\\nBackground\\nThe filters in the CNN are matrices that capture patterns in\\nthe images. It has been shown that filters in the later lay-\\ners of a CNN capture high-level concepts such as objects or\\nobject parts [Zhou et al., 2015]. Hence, this line of research\\nhas emerged, wherein the final decision-making rules are rep-\\nresented in terms of these filter outputs that learn high-level\\nconcept(s) in the images.\\nNeSyFOLD [Padalkar et al., 2024a] has emerged as a lead-\\ning framework, extracting a logic program (specifically, strat-\\nified answer set program [Baral, 2003]) from the CNN. Figure\\n1 illustrates the rule-extration pipeline of NeSyFOLD. The\\nbinarized outputs of the last convolutional layer, stored in the\\nbinarization table, serve as input to the FOLD-SE-M algo-\\nrithm [Wang and Gupta, 2024]. FOLD-SE-M then generates\\nthe raw rule-set where the truth value of that predicate is de-\\ntermined by the corresponding filter’s binarized output i.e. 0\\nor 1 (Fig. 1, bottom-right). Then the filters are matched to\\nthe concepts that they have learnt from the images using a\\nsemantic labelling algorithm that uses the images’ semantic\\nsegmentation masks (which are masks of the images with ev-\\nery pixel labelled with the concept that it belongs to) to deter-\\nmine what concept(s) the filters are most activated by in the\\nimages. The predicates in the raw rule-set are then labelled\\nas those concept(s). Then, during test-time, the images are\\npassed through the CNN and their respective filter outputs are\\nbinarized using the threshold computed after training. Based\\non the binary filter activation values, the truth value of the\\npredicates in the rule-set is determined and thus the classifi-\\ncation is made based on the rule that evaluates to true.\\nFOLD-SE-M [Wang and Gupta, 2024] is a Rule-Based\\nMachine Learning (RBML) algorithm that generates rules\\nfrom tabular data, encoding them as stratified answer set pro-\\ngrams. This means that there are no cycles through negation\\nin the rules. It uses special abx predicates to represent ex-\\nceptions, where x is a unique identifier. The algorithm in-\\ncrementally generates literals for default rules to cover pos-\\nitive examples while avoiding negative ones. It recursively\\nlearns exceptions by swapping positive and negative exam-\\nples.\\nFOLD-SE-M includes two hyperparameters: ratio,\\nwhich limits the ratio of false positives to true positives cov-\\nered by a predicate, and tail, which sets a threshold for the\\nminimum number of examples covered by a rule. Compared\\nto decision-tree classifiers, FOLD-SE-M has been shown to\\nproduce fewer, more interpretable rules with higher accuracy\\n[Wang and Gupta, 2024].\\nThe NeSyFOLD framework when used with a CNN trained\\nusing a class-specific sparse filter learning technique such as\\nElite BackProp (EBP) [Kasioumis et al., 2021] achieves state-\\nof-the-art results [Padalkar et al., 2024a]. EBP is designed to\\nassociate each class with a small number of highly responsive\\n“Elite” filters. This is achieved by employing a loss function\\n(along with the cross-entropy loss) that penalizes filters with a\\nlower probability of activation for any class, while reinforcing\\nthose with higher activation probabilities during training. As\\na result, only a few filters learn the representations for each\\nclass. The number of elite filters assigned to each class is con-\\ntrolled by the hyperparameter K. This reduces the number\\n\\nof filters that need to be binarized post-training which some-\\nwhat reduces the information loss due to fewer filter output\\nbinarizations required. However, there is no focus on opti-\\nmizing the filter outputs to a more symbolic-friendly, binary\\nform during training so there is still significant information\\nloss when a rule-set is extracted.\\n3\\nMethodology\\n3.1\\nCalculating the Sparsity Loss\\nWe now introduce our novel sparsity loss for learning class-\\nspecific filters and effective binarization of those filter outputs\\nto address the limitation of EBP discussed above.\\nComputing the Filter Probability Matrix (P)\\nThe P matrix stores the probability of activation for each fil-\\nter across all classes. It is a 2D matrix of shape (C, F), where\\nC is the total number of classes and F is the total number of\\nfilters in the last convolutional layer. We indicate 2 different\\nmethods to compute the P matrix.\\nMethod 1 (Using class-specific activation frequency):\\n1. Extract the feature maps generated by each filter from\\nthe last convolutional layer for all training images.\\n2. For each image, compute the normalized feature map\\noutput for each filter:\\nNormi,j =\\nP\\nh,w |fi,j,h,w|\\nH · W\\n,\\ni ∈[1, N], j ∈[1, F]\\nwhere H and W are the spatial dimensions of the feature\\nmap f, N is the number of training images, and F is the\\nnumber of filters.\\n3. Accumulate these norms into a class-filter matrix D of\\nshape (C, F) by summing over all images of the same\\nclass such that each row represents a class and each col-\\numn represents the cumulative norm value for each filter.\\n4. For each class, identify the top K filters with the highest\\ncumulative activations, where K is a hyperparameter.\\n5. Calculate probability P[i][j] for each filter j in class i:\\nP[i][j] =\\n\\x1a1,\\nif j ∈Top-K filters for i\\n0,\\notherwise\\n(1)\\nMethod 2 (Random Initialization): Compute the P matrix by\\nrandomly initializing K filters for each class as 1 and every\\nother filter as 0.\\nThreshold Tensor Calculation\\nThe threshold tensor is computed to determine the activation\\nthresholds for each filter. The steps are:\\n1. For each training image, calculate the L2-norm of the\\nfilter feature maps from the last convolutional layer:\\nNormi,j = ∥fi,j∥2,\\ni ∈[1, N], j ∈[1, F]\\n2. Compute the mean (µj) and standard deviation (σj) of\\nthese norms for each filter across all images.\\n3. Calculate the threshold for each filter:\\nThresholdj = h1 · µj + h2 · σj\\n(2)\\nwhere h1 and h2 are hyperparameters.\\nSparsity Loss Computation\\nDuring training, the sparsity loss is calculated as follows:\\n1. Compute the L2-norms of the filter feature maps for ev-\\nery input image n:\\nNormn,j = ∥fn,j∥2,\\nn ∈[1, N], j ∈[1, F]\\n(3)\\n2. Subtract the precomputed threshold from each filter\\nnorm:\\nAdjustedn,j = Normn,j −Thresholdj\\n(4)\\n3. Apply the sigmoid function to the adjusted norms so that\\nthe outputs are in range [0, 1]:\\nSigmoidn,j = σ(Adjustedn,j) =\\n1\\n1 + e−Adjustedn,j\\n(5)\\n4. Retrieve the filter activation probabilities for the target\\nclass of each image using the P matrix:\\nClassProbabilitiesn,j = P[class of n][j]\\n5. Define the target activations for the filters for each im-\\nage:\\nTargetn,j =\\n\\x1a1,\\nif ClassProbabilitiesn,j = 1\\n0,\\notherwise\\n(6)\\n6. Compute the Binary Cross-Entropy (BCE) loss between\\nthe predicted sigmoid activations and the target activa-\\ntions:\\nLsparsity = −\\n1\\nN · F\\nN\\nX\\nn=1\\nF\\nX\\nj=1\\n\\x14\\nTargetn,j · log(Sigmoidn,j)\\n+ (1 −Targetn,j) · log(1 −Sigmoidn,j)\\n\\x15\\n(7)\\nTotal Loss\\nThe sparsity loss is combined with the cross-entropy loss to\\nform the total Loss L:\\nL = α · Lcross-entropy + β · Lsparsity\\n(8)\\nwhere α and β are hyperparameters controlling the trade-\\noff between classification accuracy and filter sparsity.\\nThe most critical step in calculating the sparsity loss is as-\\nsigning each filter a target value of either 1 or 0 (Equation\\n(6)). This assignment frames the problem as a binary classifi-\\ncation task, where each filter is categorized as either “active”\\nor “inactive.” The Binary Cross-Entropy (BCE) loss is then\\napplied to optimize this classification which is designed for\\nsuch tasks. Thus, as the training progresses, the filter outputs\\ngradually converge toward binary values (0 or 1). This en-\\nsures minimal information loss when the outputs are finally\\nrounded to binarize them for rule-extraction.\\n\\n3.2\\nExtracting the Rule-Set and Inference\\nOnce the training is complete, the NeSyFOLD pipeline is em-\\nployed wherein each image in the train set is passed through\\nthe CNN and the last layer filter feature maps are obtained.\\nThen Equations (3) to (5) are applied in order to obtain the\\nsigmoid values which are rounded to the nearest integer (i.e.\\n0 or 1). Thus, each image is converted to a binarized vector\\nand the FOLD-SE-M algorithm is used on the full binarized\\ntrain set to obtain the rule-set.\\nAt test-time, the input images are passed through the CNN\\nto obtain the outputs of the last convolutional layer filters.\\nThe rounded sigmoid values are then computed as described\\npreviously. These values are used as truth values for the pred-\\nicates in the rule-set. The final classification is made by em-\\nploying the FOLD-SE-M toolkit’s internal interpreter to de-\\ntermine which rule is activated based on these truth values.\\n4\\nExperiments and Results\\nWe conducted experiments to address the following research\\nquestions:\\nQ1: How does altering various steps in the sparsity loss com-\\nputation affect the performance of the NeSy model?\\nQ2: What is the maximum performance gain that can be\\nachieved w.r.t.\\naccuracy and rule-set size compared to\\nNeSyFOLD-EBP, using the sparsity loss?\\nQ3: How well does this approach scale as the number of\\nclasses increases?\\nQ4: What effect does the sparsity loss have on the represen-\\ntations learned by the CNN filters?\\n[Q1, Q2, Q3] Training Strategies (TS), Performance and\\nScalability:\\nWe evaluate various training strategies, each by varying a\\nkey step in the computation of the sparsity loss. First, we\\nexplain the setup of our experiments:\\nSetup:\\nWe evaluate performance using three key metrics:\\n(1) the accuracy of the NeSy model (comprising the CNN\\nand the extracted rule-set), (2) the fidelity of the NeSy model\\nwith respect to the original CNN, and (3) the total number\\nof predicates in the rule-set (referred to as the rule-set size).\\nFidelity is defined as the proportion of predictions made by\\nthe NeSy model that match those of the original CNN, calcu-\\nlated by dividing the number of matching predictions by the\\ntotal number of images in the test set. A smaller rule-set size\\nimproves interpretability [Lage et al., 2019], hence we use\\nrule-set size as a metric of interpretability.\\nDatasets: We evaluate our approach on the same datasets as\\nNeSyFOLD-EBP (NeSyFOLD with Elite BackProp (EBP))\\n[Padalkar et al., 2024a], ensuring a fair comparison. We used\\nthe Places [Zhou et al., 2017] dataset which contains im-\\nages from various indoor and outdoor “scene” classes such\\nas “bathroom”, “bedroom”, “desert road”, “forest road” etc.\\nWe created multiple subsets from this dataset of varying num-\\nber of classes. P2 includes images from the bathroom and\\nbedroom classes, P3.1 is formed by adding the kitchen class\\nimages to P2. P5 is created by adding dining room and liv-\\ning room images to P3.1, and P10 further includes home of-\\nfice, office, waiting room, conference room, and hotel room\\nimages in addition to all classes in P5. Additionally, P3.2\\ncomprises desert road, forest road, and street images, while\\nP3.3 contains desert road, driveway, and highway images.\\nEach class has 5k images of which we made a 4k/1k train-\\ntest split for each class and we used the given validation set\\nas it is. We also used the German Traffic Sign Recognition\\nBenchmark (GTSRB) [Stallkamp et al., 2012] dataset which\\nconsists of images of various traffic signposts. This dataset\\nhas 43 classes of signposts. We used the given test set of\\n12.6k images as it is and did an 80 : 20 train-validation split\\nwhich gave roughly 21k images for the train set and 5k for\\nthe validation set.\\nHyperparameters: We employed a VGG16 CNN pretrained\\non ImageNet [Deng et al., 2009], training for 100 epochs with\\nbatch size 32. The Adam [Kingma and Ba, 2015] optimizer\\nwas used, accompanied by class weights to address data im-\\nbalance. L2 Regularization of 0.005 spanning all layers, and\\na learning rate of 5 × 10−6 was adopted. A decay factor of\\n0.5 with a 10-epoch patience was implemented. Images were\\nresized to 224×224, and hyperparameters h1 and h2 (eq. (2))\\nfor calculating threshold for binarization of kernels, were set\\nat 0.6 and 0.7 respectively. The α and β used in the final loss\\ncalculation (Eq. (8)) were set to 1 and 5 respectively. The K\\nvalue to find Top-K filters per class as described in Method\\n1, step 4 of computing the filter probability matrix (P) is set\\nto 5 for P2, P3.1, P3.2 and P3.3 and 20 for P10 and GT43\\nas they have a higher number of classes. The ratio and tail\\nhyperparameter values for rule extraction using FOLD-SE-M\\nwere set to 0.8 and 5 × 10−3 respectively.\\nNext, we discuss the various training strategies using this\\nsparsity loss function\\nTS 1: This strategy involves training the CNN for 50 epochs\\nwith the sparsity loss term set to 0. After the 50th epoch we\\ncompute the P matrix using Method 1, Top-K filters per class\\nand the thresholds for all the filters. Then, we train for 50\\nmore epochs with the sparsity loss term in effect. The intu-\\nition behind this approach is that training the CNN without\\nsparsity constraints in the initial phase should allows the fil-\\nters to naturally specialize and differentiate themselves. This\\nspecialization should result in a more informed selection of\\nthe Top-K filters, whose targets are set to 1 during the subse-\\nquent training phase. This two-step process aims to improve\\nboth accuracy and the quality of the learned representations.\\nTS 2: Here we compute the filter thresholds, P matrix using\\nMethod 1, and Top-K from the initial ImageNet pretrained\\nweights of the CNN. The sparsity loss is employed from the\\nstart along with the cross-entropy loss. This is done to under-\\nstand how enforcing sparsity constraints from the beginning\\nof training affects the accuracy and rule-set size.\\nTS 3: To understand how the choice of the Top-K filters af-\\nfects the performance, we initialize P using the random ini-\\ntialization method (Method 2).\\nSo, Top-K filters for each\\nclass are chosen as these randomly assigned filters and the\\nsparsity loss is employed from the start along with the cross-\\nentropy loss.\\nTS 4: In this strategy, we use the same configuration as TS 3,\\nbut the cross-entropy loss term is set to 0. Consequently, only\\nthe sparsity loss is optimized throughout the training. This\\n\\nP2\\nP3.1\\nP3.2\\nP3.3\\nP5\\nP10 GT43\\nMS\\nNE\\n92, 12 86, 16 91, 07 78, 23 67, 30 44, 65 85, 99 78, 36\\nTS1 94, 10 87, 17 93, 13 82, 24 61, 19 44, 50 80, 76 77, 30\\nTS2 96, 05 92, 11 94, 07 85, 18 78, 22 63, 35 91, 67 86, 24\\nTS3 98, 05 95, 07 97, 07 89, 18 75, 08 61, 29 95, 43 87, 17\\nTS4 94, 05 86, 05 93, 07 82, 11 62, 07 48, 16 90, 44 79, 14\\nTS5 94, 08 88, 16 89, 11 74, 17 56, 25 08, 01 17, 06 61, 12\\nTable 1:\\nAccuracy (blue) and the rule-set size (red) of the\\nNeSy model generated by each Training Strategy (TSx).\\nNE is\\nNeSyFOLD-EBP. The headers are various datasets and MS shows\\nthe Mean Statistics across all datasets.\\nP2\\nP3.1\\nP3.2\\nP3.3\\nP5\\nP10 GT43\\nMS\\nNE\\n97, 93 94, 87 96, 92 89, 82 85, 70 70, 49 98, 85 90, 80\\nTS1 97, 95 94, 89 96, 94 88, 85 85, 63 70, 48 98, 80 90, 79\\nTS2 97, 97 94, 94 96, 96 88, 91 84, 78 69, 66 97, 90 89, 87\\nTS3 97, 98 94, 95 96, 97 88, 89 85, 75 69, 63 98, 95 90, 87\\nTS4 50, 49 33, 30 33, 32 33, 32 20, 15 10, 07 03, 03 26, 24\\nTS5 96, 94 92, 89 96, 90 87, 75 80, 58 21, 00 94, 17 81, 60\\nTable 2: The CNN model’s Accuracy (blue) and the Fidelity (red)\\nof the NeSy mdoel generated for each Training Strategy (TSx). NE\\nis NeSyFOLD with EBP. The headers are various datasets and MS\\nshows the Mean Statistics across all datasets.\\napproach is designed to assess whether learning class-specific\\nfilters alone provides sufficient information for the FOLD-\\nSE-M algorithm to partition the feature space and generate\\neffective rules for each class.\\nTS 5: Similar to TS 3, this strategy keeps the same config-\\nuration but skips the computation of thresholds. Instead, the\\nsigmoid function is directly applied to the computed norms\\nin Eq. (3) without subtracting the thresholds. This approach\\nhelps in understanding the impact of threshold computation\\non the training process and the overall performance.\\n[Results: Q1, Q2] Performance Comparison among vari-\\nous Training Strategies and Maximum Gain: We present\\nthe accuracy of the NeSy model (blue) and the rule-set size\\n(red) for all training strategies and NeSyFOLD-EBP (NE) in\\nTable 1. Table 2 reports the original CNN model’s accuracy\\n(blue) alongside the NeSy model’s fidelity with respect to the\\nCNN (red). Accuracy and fidelity are average percentages,\\nwhile rule-set size represents the average rule-set size, all\\ncomputed over 5 runs per dataset and rounded to the near-\\nest integer. MS shows the average over all datasets for each\\nstrategy. When discussing the performance of each strategy,\\nwe omit specifying that it pertains to the NeSy model gener-\\nated by that strategy, unless explicitly referring to the original\\nCNN model.\\nTable 1 shows that TS2, TS3, and TS4 outperforms NE in\\nboth accuracy and rule-set size. TS1 achieves accuracy com-\\nparable to NE while generating a smaller rule-set, on aver-\\nage. TS5 performs the worst w.r.t. accuracy but generates the\\nsmallest rule-sets. Ideally, the accuracy-to-rule-set-size ratio\\nshould be maximized, favoring high accuracy with smaller\\nrule-sets for better interpretability.\\nTS3 stands out as the best overall, achieving 9% higher\\naccuracy and generating a rule-set that is 53% smaller than\\nNE on average. TS3 does even better than the trained CNN\\non P2, P3.1, P3.2 and P3.3 w.r.t. accuracy.\\nIn TS3, K filters are randomly assigned a probability of 1\\nper class, with the rest set to 0 in the P matrix at the start of\\ntraining. Interestingly, this approach outperforms TS2, which\\nachieves a 8% higher accuracy and a 33% reduction in rule-\\nset size compared to NE. Recall that in our experiments the\\nCNN was initialized with pretrained ImageNet weights, as is\\nstandard practice.\\nIn TS2, the Top-K filters are selected at the start of the\\ntraining based on their activation strength using feature map\\nnorms. However, the poorer performance compared to TS3\\nmay stem from the pretrained ImageNet weights, which are\\nobtained through conventional training with a cross-entropy\\nloss. This process might not yield an optimal filter selection\\nfor the sparsity loss and hence the random selection of Top-K\\nfilters in TS3 leads to better performance.\\nThis trend is even more pronounced in TS1, where the\\nCNN is initially trained for 50 epochs using only the cross-\\nentropy loss. Afterward, the Top-K filters and the P matrix\\nare computed, and the sparsity loss is activated alongside the\\ncross-entropy loss for another 50 epochs. However, since the\\ninitial filter selection is based on a CNN trained solely with\\ncross-entropy loss, the chosen Top-K filters might not be op-\\ntimal for the sparsity loss, thus leading to a suboptimal per-\\nformance.\\nIn TS4, the CNN is trained solely with the sparsity loss,\\nwithout the cross-entropy loss, using randomly initialized\\nTop-K filters. This leads to a remarkable observation: even\\nwithout cross-entropy, the NeSy model achieves a 1% ac-\\ncuracy gain and a 61% reduction in rule-set size compared\\nto NE. The absence of cross-entropy constraints allows the\\nsparsity loss to better separate filters in the latent space, opti-\\nmizing their utility as features in the binarization table. As a\\nresult, FOLD-SE-M can effectively generate rules that segre-\\ngate classes using fewer filters per class, reducing the rule-set\\nsize while maintaining high accuracy.\\nIn TS5, the thresholds are not calculated, and Top-K fil-\\nters are randomly selected. The sigmoid is applied directly to\\nthe norms of the filters without subtracting thresholds before-\\nhand. As norms are always positive, the minimum sigmoid\\nvalue becomes 0.5, limiting the representation power. This\\nmeans that the non-Top-K (both poorly and highly activating)\\nfilters can only be assigned a value of 0.5 at the minimum,\\nmaking them harder to distinguish from the Top-K filters. In\\ncontrast, subtracting the threshold enables some filter norms\\nto become negative, allowing the sigmoid to push irrelevant\\nfilters closer to 0. This facilitates true binarization, where\\nnon-relevant filters are suppressed, and preferably the Top-K\\nfilters remain closer to 1. The lack of threshold subtraction in\\nTS5 compromises its performance especially as the number\\nof classes increases.\\nNote that the accuracy of the original CNN model as de-\\nnoted in Table 2 (blue) is similar for NE and TS1-TS3. TS4\\ndoesn’t employ the cross-entropy loss so the accuracy of the\\nCNN is naturally low. In TS5 the accuracy drops as the num-\\nber of classes increases. Also note that the gap between the\\noriginal CNN model and the NeSy model in terms of accu-\\n\\nFigure 2: Activation maps of the top filters for the training strategies TS1 - TS5 for P3.1 dataset. Each row represents the top −3 images for\\nthe top filter, per class, per training strategy.\\nracy is 12% in NE but it drops to just 3% and 2% for TS3 and\\nTS2 respectively, suggesting that the information loss caused\\nby post-training binarization is greatly reduced.\\nThe NeSy models generated in both TS2 and TS3 have the\\nhighest average fidelity which is 7% higher than NE. TS4\\nshows the lowest fidelity because the CNN is not trained for\\nclassification. So, the NeSy model generated by TS4 is a\\nstand alone-model that does not follow the original model at\\nall but does give a higher accuracy than the baseline (NE) on\\naverage. TS5 shows poor fidelity because of the limited rep-\\nresentational capability as discussed earlier.\\n[Results: Q3] Scalability: TS2, TS3, and TS4 perform bet-\\nter than NE for P10 and GT43 which are datasets with 10 and\\n43 classes respectively, showing better scalability. In fact,\\nTS2 and TS3 show a 19% and 17% increase in accuracy and\\nshow a 46% and 55% decrease in rule-set size respectively\\nfor P10. Similarly for GT43, TS2 and TS3 show a 6% and\\n10% increase in accuracy and a 32% and 57% decrease in\\nrule-set size.\\n[Q4] Filter Representations:\\nSetup:For the P3.1 dataset, which includes the bathroom,\\nbedroom, and kitchen classes, we analyzed the extracted rule-\\nsets for TS1–TS5. For each class, under each training strat-\\negy, we identified the top filter by examining the rule-set and\\nselecting the filter that appeared as the first non negated pred-\\nicate in the first rule for each class. We then overlaid the\\nfeature maps of these filters onto the top three images that\\nactivated them the most. Figure 2 displays these overlays,\\nshowing three images per filter, per class, and per training\\nstrategy. Each row corresponds to a training strategy, while\\neach set of three columns represents the top three images that\\nmost activate the chosen filter for a specific class. For in-\\nstance, filters 145, 295, and 38 were selected for TS3 for the\\nbathroom, bedroom, and kitchen classes, respectively. The\\nfirst three columns show the top three images activating filter\\n145, the next three columns correspond to filter 295, and the\\nfinal three columns show the images activating filter 38.\\n[Result: Q4]: The filters typically activate for concepts rel-\\nevant to their respective classes. For instance, the bathroom\\nfilter consistently activates for toilets and sinks, the bedroom\\nfilter for beds, and the kitchen filter for stoves and cabinets.\\nNotably, TS4 filters also activate for these relevant concepts,\\neven though the CNN in this strategy was not trained with\\ncross-entropy loss for classification. This demonstrates that\\nour sparsity loss alone is sufficient to guide the filters in learn-\\ning meaningful concept representations.\\nUsing such overlays, and provided semantic segmentation\\nmasks the NeSyFOLD toolkit allows to label each predicate\\nin the rule-set with the concept(s) that it represents, thus cre-\\nating a labelled rule-set that is highly interpretable.\\nFig-\\nure 3 (top) shows one such raw rule-set generated for the\\nP3.1 dataset using TS3. Then via semantic labelling using\\nthe NeSyFOLD toolkit each predicate is mapped to the con-\\ncept(s) its corresponding filter represents as shown in Figure 3\\n(middle). Recall that the rule-set is a stratified ASP program\\nand thus an ASP interpreter such as s(CASP) [Arias et al.,\\n2018] can be used to obtain justification of an image repre-\\nsented as facts corresponding to the activated (1) binarized fil-\\nters such as {145(img).\\n134(img).} etc. Then using\\nthe query, ?-target(img, X). against a rule-set such as\\nthe one shown in Figure 3 (top), a model (i.e. answer for X\\nor class of img), as well as a justification, can be obtained\\n(Figure 3 (bottom)). Notice that the filter 398 is labelled as\\nwall3 cabinet2 signifying that this filter is activated by a\\nparticular type of wall-cabinet combination in images. Also,\\nthe numbers in the label of the predicate are just indicators\\nthat other predicates have also been labelled as similar con-\\ncepts. This showcases the advantage of interpretable methods\\n\\nFigure 3: Raw rule-set produced via TS3 for P3.1 dataset (top). La-\\nbelled rule-set produced via the NeSyFOLD toolkit (middle) and\\njustification provided by s(CASP) ASP engine for an image (bot-\\ntom).\\nas every decision made by the model can be traced in a sys-\\ntematic manner.\\n5\\nRelated Works\\nExtracting knowledge from neural networks is a well stud-\\nied topic [Andrews et al., 1995; Tickle et al., 1998] but only\\nrecently extracting rules from CNNs by binarizing the fil-\\nter outputs has gained popularity [Townsend et al., 2021;\\nTownsend et al., 2022; Padalkar et al., 2024a]. However,\\nnone of these approaches facilitate binarization of filter out-\\nputs like our method. Our approach addresses the perfor-\\nmance gap between the NeSy model and the trained CNN.\\nThere has been a lot of effort in sparsifying the weights of\\nneural networks [Ma and Niu, 2018]. However, our work\\nis concerned with sparsifying the activations or outputs of\\nthe filters. Other approaches such as Dropout [Srivastava et\\nal., 2014], Sparseout [Khan and Stavness, 2019] and Das-\\nnet [Yang et al., 2019] are techniques where the activations\\nare masked during training to induce activation sparsity. Our\\nsparsity loss does not involve any masking. Methods such\\nas ICNN [Zhang et al., 2018], ICCNN [Shen et al., 2021]\\nand learning sparse class-specific gate structures [Liang et al.,\\n2020] deal with class-specific filters but unlike our approach\\nthey do not focus on binarization of the filter outputs. Some\\nmore approaches induce sparsity in the activations layer-wise\\ni.e. a certain percentage of activations in each layer is retained\\n[Georgiadis, 2019; Kurtz et al., 2020]. We on the other hand\\ninduce class-wise sparsity.\\nPrevious research on interpreting CNNs has primarily fo-\\ncused on visualizing the outputs of CNN layers, aiming to\\nestablish relationships between input pixels and neuron out-\\nputs. Zeiler et al. [Zeiler and Fergus, 2014] employ output ac-\\ntivations as a regularizer to identify discriminative image re-\\ngions. Other studies [Denil et al., 2014; Selvaraju et al., 2017;\\nSimonyan et al., 2013], utilize gradients to perform input-\\noutput mapping. Unlike these visualization methods, our ap-\\nproach is useful for methods that generate a rule set using\\nfilter outputs.\\n6\\nConclusion and Future Work\\nIn this work, we have addressed the issue of information\\nloss due to the post-binarization of filters in rule-extraction\\nframeworks such as NeSyFOLD. We presented a novel spar-\\nsity loss that helps in learning class-specific sparse filters and\\nbinarizes the filter outputs during training itself to mitigate\\npost-binarization information loss. We evaluated five train-\\ning strategies employing the sparsity loss and compared their\\nperformance with the baseline, NeSyFOLD-EBP.\\nAs general guidelines for researchers, we recommend us-\\ning TS2 and TS3 when high fidelity to the original model\\nis required, alongside a balance between accuracy and inter-\\npretability. TS4 is best suited for scenarios where minimizing\\nthe rule-set size is a priority. Evidently TS2, TS3 and TS4 all\\noutperform the baseline both w.r.t. accuracy and the rule-set\\nsize.\\nFinally, we demonstrated that interpretable neuro-symbolic\\nmethods can achieve accuracy levels within 3% - 4% of the\\noriginal CNN model, without compromising on interpretabil-\\nity. This establishes these methods as viable and interpretable\\nalternatives to black-box CNN models.\\nCurrently, our approach is tailored to CNN models, and\\nit would be intriguing to investigate whether a similar spar-\\nsity loss function could be adapted for Vision Transformers.\\nAlthough the challenge lies in the absence of filters that di-\\nrectly capture concepts, sparse autoencoders could be used to\\nextract relevant concepts from attention layers [Cunningham\\net al., 2023]. Another promising direction is integrating the\\nsymbolic rule-set into the training loop to further mitigate in-\\nformation loss by leveraging soft decision tree like structure\\nfor gradient backpropogation [Irsoy et al., 2012]. We aim\\nto advance the development of interpretable neuro-symbolic\\nmodels that match or even surpass the performance of black-\\nbox neural models, continuing our quest for models that com-\\nbine interpretability and accuracy.\\nReferences\\n[Andrews et al., 1995] Robert Andrews, Joachim Diederich,\\nand Alan B. Tickle.\\nSurvey and critique of techniques\\nfor extracting rules from trained artificial neural net-\\nworks. Knowledge-Based Systems, 8(6):373–389, 1995.\\nKnowledge-based neural networks.\\n\\n[Arias et al., 2018] Joaqu´ın Arias, Manuel Carro, Elmer\\nSalazar, Kyle Marple, and Gopal Gupta. Constraint answer\\nset programming without grounding. Theory and Practice\\nof Logic Programming, 18(3-4):337–354, 2018.\\n[Baral, 2003] Chitta Baral. Knowledge representation, rea-\\nsoning and declarative problem solving. Cambridge Uni-\\nversity Press, 2003.\\n[Cunningham et al., 2023] Hoagy\\nCunningham,\\nAidan\\nEwart, Logan Riggs, Robert Huben, and Lee Sharkey.\\nSparse autoencoders find highly interpretable features in\\nlanguage models. arXiv preprint arXiv:2309.08600, 2023.\\n[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-\\nJia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\\nhierarchical image database. In Proc. CVPR, pages 248–\\n255. Ieee, 2009.\\n[Denil et al., 2014] Misha Denil, Alban Demiraj, and Nando\\nde Freitas. Extraction of salient sentences from labelled\\ndocuments. ArXiv, abs/1412.6815, 2014.\\n[Georgiadis, 2019] Georgios Georgiadis. Accelerating con-\\nvolutional neural networks via activation map compres-\\nsion.\\nIn Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, pages 7085–\\n7095, 2019.\\n[Irsoy et al., 2012] Ozan Irsoy, O. T. Yildiz, and Ethem Al-\\npaydin. Soft decision trees. Proceedings of the 21st Inter-\\nnational Conference on Pattern Recognition (ICPR2012),\\npages 1819–1822, 2012.\\n[Kanagaraj et al., 2021] Nitin\\nKanagaraj,\\nDavid\\nHicks,\\nAyush Goyal, Sanju Mishra Tiwari, and Ghanapriya\\nSingh.\\nDeep learning using computer vision in self\\ndriving cars for lane and traffic sign detection.\\nInter-\\nnational Journal of System Assurance Engineering and\\nManagement, 12, 05 2021.\\n[Kasioumis et al., 2021] Theodoros\\nKasioumis,\\nJoe\\nTownsend,\\nand Hiroya Inakoshi.\\nElite backprop:\\nTraining sparse interpretable neurons.\\nIn International\\nWorkshop on Neural-Symbolic Learning and Reasoning,\\n2021.\\n[Khan and Stavness, 2019] Najeeb Khan and Ian Stavness.\\nSparseout: Controlling sparsity in deep networks.\\nIn\\nCanadian conference on artificial intelligence, pages 296–\\n307. Springer, 2019.\\n[Kingma and Ba, 2015] Diederik P. Kingma and Jimmy Ba.\\nAdam: A method for stochastic optimization.\\nIn Proc.\\nICLR, 2015.\\n[Ko and Kwak, 2012] Byoungchul Ko and Sooyeong Kwak.\\nSurvey of computer vision-based natural disaster warning\\nsystems. Optical Engineering, 51:0901–, 07 2012.\\n[Kurtz et al., 2020] Mark\\nKurtz,\\nJustin\\nKopinsky,\\nRati\\nGelashvili, Alexander Matveev, John Carr, Michael Goin,\\nWilliam Leiserson, Sage Moore, Nir Shavit, and Dan Al-\\nistarh. Inducing and exploiting activation sparsity for fast\\ninference on deep neural networks. In International Con-\\nference on Machine Learning, pages 5533–5543. PMLR,\\n2020.\\n[Lage et al., 2019] Isaac Lage, Emily Chen, Jeffrey He,\\nMenaka Narayanan, Been Kim, Samuel J Gershman, and\\nFinale Doshi-Velez. Human evaluation of models built for\\ninterpretability. In Proc. HCOMP, volume 7, pages 59–67,\\n2019.\\n[LeCun et al., 1989] Yann\\nLeCun,\\nBernhard\\nE.\\nBoser,\\nJohn S. Denker, Donnie Henderson, Richard E. Howard,\\nWayne E. Hubbard, and Lawrence D. Jackel. Backpropa-\\ngation applied to handwritten zip code recognition. Neural\\nComput., 1(4):541–551, 1989.\\n[Li et al., 2022] Xuhong Li, Haoyi Xiong, Xingjian Li, Xu-\\nanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, and Dejing Dou.\\nInterpretable deep learning: Interpretation, interpretabil-\\nity, trustworthiness, and beyond. Knowledge and Informa-\\ntion Systems, 64(12):3197–3234, 2022.\\n[Liang et al., 2020] Haoyu Liang, Zhihao Ouyang, Yuyuan\\nZeng, Hang Su, Zihao He, Shu-Tao Xia, Jun Zhu, and\\nBo Zhang. Training interpretable convolutional neural net-\\nworks by differentiating class-specific filters. In Computer\\nVision – ECCV 2020: 16th European Conference, Glas-\\ngow, UK, August 23–28, 2020, Proceedings, Part II, page\\n622–638, Berlin, Heidelberg, 2020. Springer-Verlag.\\n[Ma and Niu, 2018] Rongrong Ma and Lingfeng Niu. A sur-\\nvey of sparse-learning methods for deep neural networks.\\nIn 2018 IEEE/WIC/ACM International Conference on Web\\nIntelligence (WI), pages 647–650. IEEE, 2018.\\n[Ngan et al., 2022] Kwun Ho Ngan, Artur D’Avila Garcez,\\nand Joseph Townsend.\\nExtracting meaningful high-\\nfidelity knowledge from convolutional neural networks. In\\n2022 International Joint Conference on Neural Networks\\n(IJCNN), pages 1–17, 2022.\\n[Padalkar et al., 2024a] Parth Padalkar, Huaduo Wang, and\\nGopal Gupta. Nesyfold: A framework for interpretable im-\\nage classification. In Proc. AAAI, pages 4378–4387. AAAI\\nPress, 2024.\\n[Padalkar et al., 2024b] Parth Padalkar, Huaduo Wang, and\\nGopal Gupta.\\nUsing logic programming and kernel-\\ngrouping for improving interpretability of convolutional\\nneural networks. In Proc. PADL, volume 14512 of LNCS,\\npages 134–150. Springer, 2024.\\n[Padalkar et al., 2024c] Parth Padalkar, Natalia ´Slusarz, Eka-\\nterina Komendantskaya, and Gopal Gupta. A neurosym-\\nbolic framework for bias correction in convolutional neu-\\nral networks. Theory and Practice of Logic Programming,\\n24(4):644–662, 2024.\\n[R¨auker et al., 2023] Tilman R¨auker, Anson Ho, Stephen\\nCasper, and Dylan Hadfield-Menell.\\nToward transpar-\\nent ai: A survey on interpreting the inner structures of\\ndeep neural networks. In 2023 ieee conference on secure\\nand trustworthy machine learning (satml), pages 464–483.\\nIEEE, 2023.\\n[Selvaraju et al., 2017] Ramprasaath R Selvaraju, Michael\\nCogswell, Abhishek Das, Ramakrishna Vedantam, Devi\\nParikh, and Dhruv Batra. Grad-cam: Visual explanations\\nfrom deep networks via gradient-based localization.\\nIn\\n\\nProceedings of the IEEE international conference on com-\\nputer vision, pages 618–626, 2017.\\n[Shen et al., 2021] Wen Shen, Zhihua Wei, Shikun Huang,\\nBinbin Zhang, Jiaqi Fan, Ping Zhao, and Quanshi\\nZhang. Interpretable compositional convolutional neural\\nnetworks. arXiv preprint arXiv:2107.04474, 2021.\\n[Simonyan et al., 2013] Karen Simonyan, Andrea Vedaldi,\\nand Andrew Zisserman.\\nDeep inside convolutional\\nnetworks:\\nVisualising image classification models and\\nsaliency maps. CoRR, abs/1312.6034, 2013.\\n[Srivastava et al., 2014] Nitish Srivastava, Geoffrey Hinton,\\nAlex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks\\nfrom overfitting.\\nThe journal of machine learning re-\\nsearch, 15(1):1929–1958, 2014.\\n[Stallkamp et al., 2012] J.\\nStallkamp,\\nM.\\nSchlipsing,\\nJ. Salmen, and C. Igel. Man vs. computer: Benchmarking\\nmachine learning algorithms for traffic sign recognition.\\nNeural Networks, 32:323–332, 2012.\\nSelected Papers\\nfrom IJCNN 2011.\\n[Sun et al., 2016] W. Sun, Bin Zheng, and Wei Qian. Com-\\nputer aided lung cancer diagnosis with deep learning algo-\\nrithms. In SPIE Medical Imaging, 2016.\\n[Tickle et al., 1998] A.B. Tickle, R. Andrews, M. Golea, and\\nJ. Diederich. The truth will come to light: directions and\\nchallenges in extracting the knowledge embedded within\\ntrained artificial neural networks. IEEE Transactions on\\nNeural Networks, 9(6):1057–1068, 1998.\\n[Townsend et al., 2021] Joe\\nTownsend,\\nTheodoros\\nKa-\\nsioumis, and Hiroya Inakoshi. Eric: Extracting relations\\ninferred from convolutions. In Computer Vision – ACCV,\\npages 206–222, Cham, 2021. Springer International.\\n[Townsend et al., 2022] Joe Townsend, Mateusz Kudla, Ag-\\nnieszka Raszkowska, and Theodoros Kasiousmis. On the\\nexplainability of convolutional layers for multi-class prob-\\nlems. In Combining Learning and Reasoning: Program-\\nming Languages, Formalisms, and Representations, 2022.\\n[Wang and Gupta, 2024] Huaduo Wang and Gopal Gupta.\\nFOLD-SE: an efficient rule-based machine learning algo-\\nrithm with scalable explainability. In Proc. PADL, volume\\n14512 of LNCS, pages 37–53. Springer, 2024.\\n[Yang et al., 2019] Qing Yang,\\nJiachen Mao,\\nZuoguan\\nWang, and Hai Li. Dasnet: Dynamic activation sparsity\\nfor neural network efficiency improvement. In 2019 IEEE\\n31st International Conference on Tools with Artificial In-\\ntelligence (ICTAI), pages 1401–1405. IEEE, 2019.\\n[Zeiler and Fergus, 2014] Matthew D Zeiler and Rob Fergus.\\nVisualizing and understanding convolutional networks. In\\nEuropean conference on computer vision, pages 818–833.\\nSpringer, 2014.\\n[Zhang et al., 2018] Quanshi Zhang, Ying Nian Wu, and\\nSong-Chun Zhu. Interpretable convolutional neural net-\\nworks. In Proceedings of the IEEE conference on com-\\nputer vision and pattern recognition, pages 8827–8836,\\n2018.\\n[Zhang et al., 2021] Yu Zhang, Peter Tiˇno, Aleˇs Leonardis,\\nand Ke Tang. A survey on neural network interpretability.\\nIEEE Transactions on Emerging Topics in Computational\\nIntelligence, 5(5):726–742, 2021.\\n[Zhou et al., 2015] Bolei\\nZhou,\\nAditya\\nKhosla,\\n`Agata\\nLapedriza, Aude Oliva, and Antonio Torralba. Object de-\\ntectors emerge in deep scene cnns. In ICLR Conference\\nTrack Proceedings, 2015.\\n[Zhou et al., 2017] Bolei Zhou, Agata Lapedriza, Aditya\\nKhosla, Aude Oliva, and Antonio Torralba. Places: A 10\\nmillion image database for scene recognition, 2017.\\n',\n",
       "  'metadata': {'authors': ['Parth Padalkar',\n",
       "    'Jaeseong Lee',\n",
       "    'Shiyi Wei',\n",
       "    'Gopal Gupta'],\n",
       "   'published': datetime.datetime(2025, 1, 28, 3, 22, 23, tzinfo=datetime.timezone.utc)}},\n",
       " {'title': 'Incremental Boosting Convolutional Neural Network for Facial Action Unit Recognition',\n",
       "  'text': 'Incremental Boosting Convolutional Neural Network\\nfor Facial Action Unit Recognition\\nShizhong Han, Zibo Meng, Ahmed Shehab Khan, Yan Tong\\nDepartment of Computer Science & Engineering, University of South Carolina, Columbia, SC\\n{han38, mengz, akhan}@email.sc.edu, tongy@cse.sc.edu\\nAbstract\\nRecognizing facial action units (AUs) from spontaneous facial expressions is still\\na challenging problem. Most recently, CNNs have shown promise on facial AU\\nrecognition. However, the learned CNNs are often overﬁtted and do not gener-\\nalize well to unseen subjects due to limited AU-coded training images. We pro-\\nposed a novel Incremental Boosting CNN (IB-CNN) to integrate boosting into\\nthe CNN via an incremental boosting layer that selects discriminative neurons\\nfrom the lower layer and is incrementally updated on successive mini-batches. In\\naddition, a novel loss function that accounts for errors from both the incremen-\\ntal boosted classiﬁer and individual weak classiﬁers was proposed to ﬁne-tune\\nthe IB-CNN. Experimental results on four benchmark AU databases have demon-\\nstrated that the IB-CNN yields signiﬁcant improvement over the traditional CNN\\nand the boosting CNN without incremental learning, as well as outperforming the\\nstate-of-the-art CNN-based methods in AU recognition. The improvement is more\\nimpressive for the AUs that have the lowest frequencies in the databases.\\n1\\nIntroduction\\nFacial behavior is a powerful means to express emotions and to perceive the intentions of a hu-\\nman. Developed by Ekman and Friesen [1], the Facial Action Coding System (FACS) describes\\nfacial behavior as combinations of facial action units (AUs), each of which is anatomically related\\nto the contraction of a set of facial muscles. In addition to applications in human behavior analy-\\nsis, an automatic AU recognition system has great potential to advance emerging applications in\\nhuman-computer interaction (HCI), such as online/remote education, interactive games, and intelli-\\ngent transportation, as well as to push the frontier of research in psychology.\\nRecognizing facial AUs from spontaneous facial expressions is challenging because of subtle facial\\nappearance changes, free head movements, and occlusions, as well as limited AU-coded training\\nimages. As elaborated in the survey papers [2, 3], a number of approaches have been developed\\nto extract features from videos or static images to characterize facial appearance or geometrical\\nchanges caused by target AUs. Most of them employed hand-crafted features, which, however, are\\nnot designed and optimized for facial AU recognition. Most recently, CNNs have achieved incredible\\nsuccess in different applications such as object detection and categorization, video analysis, and have\\nshown promise on facial expression and AU recognition [4, 5, 6, 7, 8, 9, 10].\\nCNNs contain a large number of parameters, especially as the network becomes deeper. To achieve\\nsatisfactory performance, a large number of training images are required and a mini-batch strategy\\nis used to deal with large training data, where a small batch of images are employed in each iteration.\\nIn contrast to the millions of training images employed in object categorization and detection, AU-\\ncoded training images are limited and usually collected from a small population, e.g., 48,000 images\\nfrom 15 subjects in the FERA2015 SEMAINE database [11], and 130,814 images from 27 subjects\\nin Denver Intensity of Spontaneous Facial Action (DISFA) database [12]. As a result, the learned\\nCNNs are often overﬁtted and do not generalize well to unseen subjects.\\nBoosting, e.g., AdaBoost, is a popular ensemble learning technique, which combines many “weak”\\nclassiﬁers and has been demonstrated to yield better generalization performance in AU recogni-\\ntion [13]. Boosting can be integrated into the CNN such that discriminative neurons are selected and\\nactivated in each iteration of CNN learning. However, the boosting CNN (B-CNN) can overﬁt due\\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\\narXiv:1707.05395v1  [cs.CV]  17 Jul 2017\\n\\nFigure 1: An overview of Incremental Boosting CNN. An incremental boosted classiﬁer is trained iteratively.\\nOutputs of the FC layer are employed as input features and a subset of features (the blue nodes) are selected\\nby boosting. The selected features in the current iteration are combined with those selected previously (the red\\nnodes) to form an incremental strong classiﬁer. A loss is calculated based on the incremental classiﬁer and\\npropagated backward to ﬁne-tune the CNN parameters. The gray nodes are inactive and thus, not selected by\\nthe incremental strong classiﬁer. Given a testing image, features are calculated via the CNN and fed to the\\nboosted classiﬁer to predict the AU label. Best viewed in color.\\nto the limited training data in each mini-batch. Furthermore, the information captured in previous\\niteration/batch cannot be propagated, i.e., a new set of weak classiﬁers is selected in every iteration\\nand the weak classiﬁers learned previously are discarded.\\nInspired by incremental learning, we proposed a novel Incremental Boosting CNN (IB-CNN), which\\naims to accumulate information in B-CNN learning when new training samples appear. As shown\\nin Figure 1, a batch of images is employed in each iteration of CNN learning. The outputs of the\\nfully-connected (FC) layer are employed as features; a subset of features (the blue nodes), which\\nis discriminative for recognizing the target AU in the current batch, is selected by boosting. Then,\\nthese selected features are combined with the ones selected previously (the red nodes) to form an\\nincremental strong classiﬁer. The weights of active features, i.e., both the blue and the red nodes,\\nare updated such that the features selected most of the time have higher weights. Finally, a loss,\\ni.e., the overall classiﬁcation error from both weak classiﬁers and the incremental strong classiﬁer,\\nis calculated and backpropagated to ﬁne-tune the CNN iteratively. The proposed IB-CNN has a\\ncomplex decision boundary due to boosting and is capable of alleviating the overﬁtting problem for\\nthe mini-batches by taking advantage of incremental learning.\\nIn summary, this paper has three major contributions. (1) Feature selection and classiﬁcation are\\nintegrated with CNN optimization in a boosting CNN framework. (2) A novel incremental boosted\\nclassiﬁer is updated iteratively by accumulating information from multiple batches. (3) A novel loss\\nfunction, which considers the overall classiﬁcation error of the incremental strong classiﬁer and\\nindividual classiﬁcation errors of weak learners, is developed to ﬁne-tune the IB-CNN.\\nExperimental results on four benchmark AU-coded databases, i.e., Cohn-Kanade (CK) [25] databse,\\nFERA2015 SEMAINE database [11], FERA2015 BP4D database [11], and Denver Intensity of\\nSpontaneous Facial Action (DISFA) database [12] have demonstrated that the proposed IB-CNN\\nsigniﬁcantly outperforms the traditional CNN model as well as the state-of-the-art CNN-based meth-\\nods for AU recognition. Furthermore, the performance improvement of the infrequent AUs is more\\nimpressive, which demonstrates that the proposed IB-CNN is capable of improving CNN learning\\nwith limited training data. In addition, the performance of IB-CNN is not sensitive to the number of\\nneurons in the FC layer and the learning rate, which are favored traits in CNN learning.\\n2\\nRelated Work\\nAs detailed in the survey papers [2, 3], various human-designed features are adopted in recogniz-\\ning facial expressions and AUs including Gabor Wavelets [13], Local Binary Patterns (LBP) [14],\\nHistogram of Oriented Gradients (HOG) [15], Scale Invariant Feature Transform (SIFT) fea-\\ntures [16], Histograms of Local Phase Quantization (LPQ) [17], and their spatiotemporal exten-\\nsions [17, 18, 19]. Recently, feature learning approaches including sparse coding [20] and deep\\nlearning [4, 5, 6, 7, 8, 9, 10, 21] have been devoted to recognizing facial expressions and AUs.\\nAmong the feature learning based methods, CNNs [4, 5, 6, 7, 8, 9, 10] have attracted increasing\\nattention. Gudi et al. [9] used a pre-processing method with local and global contrast normalization\\n2\\n\\nto improve the inputs of CNNs. Fasel [4] employed multi-size convolutional ﬁlters to learn multi-\\nscale features. Liu et al [7] extracted spatiotemporal features using the 3D CNN. Jung et al. [8]\\njointly ﬁne-tuned temporal appearance and geometry features. Jaiswal and Valstar [10] integrated\\nbi-directional long-term memory neural networks with the CNN to extract temporal features.\\nMost of CNN-based methods make decisions using inner product of the FC layer. A few approaches\\ndeveloped new objective functions to improve recognition performance. Tang [22, 6] replaced the\\nsoftmax loss function with an SVM for optimization. Hinton et al. [23] utilized a dropout technique\\nto reduce overﬁtting by dropping out some neuron activations from the previous layer, which can\\nbe seen as an ensemble of networks sharing the same weights. However, the dropout process is\\nrandom regardless the discriminative power of individual neurons. In contrast, the proposed IB-CNN\\neffectively selects the more discriminative neurons and drops out noisy or redundant neurons.\\nMedera and Babinec [24] adopted incremental learning using multiple CNNs trained individually\\nfrom different subsets and additional CNNs are trained given new samples. Then, the prediction\\nis calculated by weighted majority-voting of the outputs of all CNNs. However, each CNN may\\nnot have sufﬁcient training data, which is especially true with limited AU-coded data. Different\\nfrom [24], the IB-CNN has only one CNN trained along with an incremental strong classiﬁer, where\\nweak learners are updated over time by accumulating information from multiple batches. Liu et\\nal. [21] proposed a boosted deep belief network for facial expression recognition, where each weak\\nclassiﬁer is learned exclusively from an image patch. In contrast, weak classiﬁers are selected from\\nan FC layer in the proposed IB-CNN and thus, learned from the whole face.\\n3\\nMethodology\\nAs illustrated in Figure 1, an IB-CNN model is proposed to integrate boosting with the CNN at the\\ndecision layer with an incremental boosting algorithm, which selects and updates weak learners over\\ntime as well as constructs an incremental strong classiﬁer in an online learning manner. There are\\nthree major steps for incremental boosting: selecting and activating neurons (blue nodes) from the\\nFC layer by boosting, combining the activated neurons from different batches (blue and red nodes)\\nto form an incremental strong classiﬁer, and ﬁne-tuning the IB-CNN by minimizing the proposed\\nloss function. In the following, we start with a brief review of CNNs and then, describe the three\\nsteps of incremental boosting in detail.\\n3.1\\nA Brief Review of CNNs\\nA CNN consists of a stack of layers such as convolutional layers, pooling layers, rectiﬁcation layers,\\nFC layers, and a decision layer and transforms the input data into a highly nonlinear representation.\\nIdeally, learned ﬁlters should activate the image patches related to the recognition task, i.e., detecting\\nAUs in this work. Neurons in an FC layer have full connections with all activations in the previous\\nlayer. Finally, high-level reasoning is done at the decision layer, where the number of outputs is\\nequal to the number of target classes. The score function used by the decision layer is generally\\nthe inner product of the activations in the FC layer and the corresponding weights. During CNN\\ntraining, a loss layer is employed after the decision layer to specify how to penalize the deviations\\nbetween the predicted and true labels, where different types of loss functions have been employed,\\nsuch as softmax, SVM, and sigmoid cross entropy. In this paper, we substitute the inner-product\\nscore function with a boosting score function to achieve a complex decision boundary.\\n3.2\\nBoosting CNN\\nIn a CNN, a mini-batch strategy is often used to handle large training data. Let X = [x1, ..., xM]\\nbe the activation features of a batch with M training images, where the dimension of the activation\\nfeature vector xi is K, and y = [y1, ..., yM], yi ∈{−1, 1} is a vector storing the ground truth\\nlabels. With the boosting algorithm, the prediction is calculated by a strong classiﬁer H(·) that is\\nthe weighted summation of weak classiﬁers h(·) as follows:\\nH(xi) =\\nK\\nX\\nj=1\\nαjh(xij, λj); h(xij, λj) =\\nf(xij, λj)\\np\\nf(xij, λj)2 + η2\\n(1)\\nwhere xij ∈xi is the jth activation feature of the ith image. Each feature corresponds to a candidate\\nweak classiﬁer h(xij, λj) with output in the range of (-1,1).\\nf(·)\\n√\\nf(·)2+η2 is used to simulate a sign(·)\\nfunction to compute the derivative for gradient descent optimization. In this work, f(xij, λj) ∈R\\nis deﬁned as a one-level decision tree (a decision stump) with the threshold of λj, which has been\\nwidely used in AdaBoost. The parameter η in Eq. 1 is employed to control the slope of function\\n3\\n\\nf(·)\\n√\\nf(·)2+η2 and can be set according to the distribution of f(·) as η = σ\\nc , where σ is the standard\\ndeviation of f(·) and c is a constant. In this work, η is empirically set to σ\\n2 . αj ≥0 is the weight of\\nthe jth weak classiﬁer and PK\\nj=1 αj = 1. When αj = 0, the corresponding neuron is inactive and\\nwill not go through the feedforward and backpropagation process.\\nTraditional boosting algorithms only consider the loss of the strong classiﬁer, which can be domi-\\nnated by some weak classiﬁers with large weights, potentially leading to overﬁtting. To account for\\nclassiﬁcation errors from both the strong classiﬁer and the individual classiﬁers, the loss function is\\ndeﬁned as the summation of a strong-classiﬁer loss and a weak-classiﬁer loss as follows:\\nεB = βεB\\nstrong + (1 −β)εweak\\n(2)\\nwhere β ∈[0, 1] balances the strong-classiﬁer loss and the weak-classiﬁer loss.\\nThe strong-classiﬁer loss is deﬁned as the Euclidean distance between the prediction and the\\ngroundtruth label:\\nεB\\nstrong = 1\\nM\\nM\\nX\\ni=1\\n(H(xi) −yi)2\\n(3)\\nThe weak-classiﬁer loss is deﬁned as the summation of the individual losses of all weak classiﬁers:\\nεweak =\\n1\\nMN\\nM\\nX\\ni=1\\nX\\n1≤j≤K\\nαj >0\\n\\x02\\nh(xij, λj) −yi\\n\\x032\\n(4)\\nwhere the constraint αj > 0 excludes inactive neurons when calculating the loss.\\nDriven by the loss εB deﬁned in Eq. 2, the B-CNN can be iteratively ﬁne-tuned by backpropagation\\nas illustrated in the top of Figure 2. However, the information captured previously, e.g., the weights\\nand thresholds of the active neurons, is discarded for a new batch. Due to limited data in each mini-\\nbatch, the trained B-CNN can be overﬁtted.\\n3.3\\nIncremental Boosting\\nFigure 2: A comparison of the IB-CNN and the B-CNN structures. For clarity, the illustration of IB-CNN\\nor B-CNN starts from the FC layer (the cyan nodes). The blue nodes are active nodes selected in the current\\niteration; the red nodes are the active nodes selected from previous iterations; and the gray nodes are inactive.\\nIncremental learning can help to improve the prediction performance and to reduce overﬁtting. As\\nillustrated in the bottom of Figure 2, both of the blue nodes selected in the current iteration and the\\nred nodes selected previously are incrementally combined to form an incremental strong classiﬁer\\nHt\\nI at the tth iteration:\\nHt\\nI(xi) = (t −1)Ht−1\\nI\\n(xi) + Ht(xi)\\nt\\n(5)\\nwhere Ht−1\\nI\\n(xi) is the incremental strong classiﬁer obtained at the (t −1)th iteration; and Ht(xi)\\nis the boosted strong classiﬁer estimated in the current iteration.\\nSubstituting Eq. 1 into Eq. 5, we have\\nHt\\nI(xi) =\\nK\\nX\\nj=1\\n(t −1)αt−1\\nj\\n+ αt\\nj\\nt\\nht(xij; λj)\\n(6)\\nwhere αt\\nj is calculated in the tth iteration by boosting. As shown in Figure 3, ht−1(·) has been\\nupdated to ht(·) by updating the threshold λt−1\\nj\\nto λt\\nj. If the jth weak classiﬁer was not selected\\n4\\n\\nAlgorithm 1 Incremental Boosting Algorithm for the IB-CNN\\nInput: The number of iterations (mini-batches) T and activation features X with the size of M ×K,\\nwhere M is the number of images in a mini-batch and K is the dimension of the activation\\nfeature vector for one image.\\n1: for each input activation j from 1 to K do\\n2:\\nα1\\nj = 0\\n3: end for\\n4: for each mini-batch t from 1 to T do\\n5:\\nFeed-forward to the FC layer;\\n6:\\nSelect active features by boosting and calculate weights αt based on the standard AdaBoost;\\n7:\\nUpdate the incremental strong classiﬁer as Eq. 6;\\n8:\\nCalculate the overall loss of IB-CNN as Eq. 8;\\n9:\\nBackpropagate the loss based on Eq. 9 and Eq. 10;\\n10:\\nContinue backpropagation to lower layers.\\n11: end for\\nbefore, λt\\nj is estimated in the tth iteration by boosting. Otherwise, λt\\nj will be updated from the\\nprevious iteration after backpropagation as follows:\\nλt\\nj = λt−1\\nj\\n−γ∇∂εHI\\n∂λt−1\\nj\\n(7)\\n...\\n...\\n...\\nHI\\nt\\nFigure 3: An illustration of constructing the incremen-\\ntal strong classiﬁer. Squares represent neuron activa-\\ntions. The gray nodes are inactive; while the blue and\\nred nodes are active nodes selected in the current itera-\\ntion and previous iterations, respectively.\\nwhere γ is the learning rate.\\nThen, the incremental strong classiﬁer Ht\\nI is up-\\ndated over time. As illustrated in Figure 3, if a\\nneuron is activated in the current iteration, the\\ncorresponding weight will increase; otherwise,\\nit will decrease. The summation of weights of\\nall weak classiﬁers will be normalized to 1.\\nHence, the weak classiﬁers selected most of the\\ntime, i.e., effective for most of mini-batches,\\nwill have higher weights. Therefore, the overall\\nloss of IB-CNN is calculated as\\nεIB = βεIB\\nstrong + (1 −β)εweak\\n(8)\\nwhere εIB\\nstrong =\\n1\\nM\\nPM\\ni=1(HI(xi) −yi)2.\\nCompared to the B-CNN, the IB-CNN exploits the information from all mini-batches. For test-\\ning, IB-CNN uses the incremental strong classiﬁer, while the B-CNN employs the strong classiﬁer\\nlearned from the last iteration.\\n3.4\\nIB-CNN Fine-tuning\\nA stochastic gradient decent method is utilized for ﬁne-tuning the IB-CNN, i.e., updating IB-CNN\\nparameters, by minimizing the loss in Eq. 8. The decent directions for xij and λj can be calculated\\nas follows:\\n∂εIB\\n∂xij = β ∂εIB\\nstrong\\n∂HI(xi)\\n∂HI(xi)\\n∂xij\\n+ (1 −β)\\n∂εIB\\nweak\\n∂h(xij; λj)\\n∂h(xij; λj)\\n∂xij\\n(9)\\n∂εIB\\n∂λj =\\nM\\nX\\ni=1\\nβ ∂εIB\\nstrong\\n∂HI(xi)\\n∂HI(xi)\\n∂λj\\n+ (1 −β)\\nM\\nX\\ni=1\\n∂εIB\\nweak\\n∂h(xij; λj)\\n∂h(xij; λj)\\n∂λj\\n(10)\\nwhere ∂εIB\\n∂xij and ∂εIB\\n∂λj are only calculated for the active nodes of incremental boosting (the red and\\nblue nodes in Figure 3). ∂εIB\\n∂xij can be further backpropagated to the lower FC layers and convolu-\\ntional layers. The incremental boosting algorithm for the IB-CNN is summarized in Algorithm 1.\\n4\\nExperiments\\nTo evaluate effectiveness of the proposed IB-CNN model, extensive experiments have been con-\\nducted on four benchmark AU-coded databases. The CK database [25] contains 486 image se-\\nquences from 97 subjects and has been widely used for evaluating the performance of AU recog-\\nnition. In addition, 14 AUs were annotated frame-by-frame [30] for training and evaluation. The\\nFERA2015 SEMAINE database [11] contains 6 AUs and 31 subjects with 93,000 images. The\\nFERA2015 BP4D database [11] has 11 AUs and 41 subjects with 146,847 images. The DISFA\\ndatabase [12] has 12 labeled AUs and 27 subjects with 130,814 images.\\n5\\n\\n4.1\\nPre-Processing\\nFace alignment is conducted to reduce variation in face scale and in-plane rotation across different\\nfacial images. Speciﬁcally, the face regions are aligned based on three ﬁducial points: the centers of\\nthe two eyes and the mouth, and scaled to a size of 128 × 96. In order to alleviate face pose vari-\\nations, especially out-of-plane rotations, face images are further warped to a frontal view based on\\nlandmarks that are less affected by facial expressions including landmarks along the facial contour,\\ntwo eye centers, the nose tip, the mouth center, and on the forehead. A total of 23 landmarks that\\nare less affected by facial muscle movements are selected as control points to warp the face region\\nto the mean facial shape calculated from all images 1.\\nTime sequence normalization is used to reduce identity-related information and highlight appearance\\nand geometrical changes due to activation of AUs. Particularly, each image is normalized based on\\nthe mean and the standard deviation calculated from a short video sequence containing at least 800\\ncontinuous frames at a frame rate of 30fps 2.\\n4.2\\nCNN Implementation Details\\nThe proposed IB-CNN is implemented based on a modiﬁcation of cifar10_quick in Caffe [28]. As\\nillustrated in Figure 1, the preprocessed facial images are fed into the network as input. The IB-CNN\\nconsists of three stacked convolutional layers with activation functions, two maxpooling layers, an\\nFC layer, and the proposed IB layer to predict the AU label. Speciﬁcally, the ﬁrst two convolutional\\nlayers have 32 ﬁlters with a size of 5 × 5 and a stride of 1. Then, the output feature maps are sent\\nto a rectiﬁed layer followed by the maxpooling layer with a downsampling stride of 3. The last\\nconvolutional layer has 64 ﬁlters with a size of 5 × 5, and the output 9 × 5 feature maps are fed\\ninto an FC layer with 128 nodes. The outputs of the FC layer are sent to the proposed IB layer.\\nThe stochastic gradient descent, with a momentum of 0.9 and a mini-batch size of 100, is used for\\ntraining the CNN for each target AU.\\n4.3\\nExperimental Results\\nTo demonstrate effectiveness of the proposed IB-CNN, two baseline methods are employed for com-\\nparison. The ﬁrst method, denoted as CNN, is a traditional CNN model with a sigmoid cross entropy\\ndecision layer. The second method, denoted as B-CNN, is the boosting CNN described in Section 3.2.\\nBoth CNN and B-CNN have the same architecture as the IB-CNN with different decision layers.\\nPerformance evaluation on the SEMAINE database: All the models compared were trained\\non the training set and evaluated on the validation set. The training-testing process was repeated\\n5 times. The mean and standard deviation of F1 score and two-alternative forced choice (2AFC)\\nscore are calculated from the 5 runs for each target AU. As shown in Table 1, the proposed\\nIB-CNN outperforms the traditional CNN in term of the average F1 score (0.416 vs 0.347) and the\\naverage 2AFC score (0.775 vs 0.735). Not surprisingly, IB-CNN also beats B-CNN obviously: the\\naverage F1 score increases from 0.310 (B-CNN) to 0.416 (IB-CNN) and the average 2AFC score\\nincreases from 0.673 (B-CNN) to 0.775 (IB-CNN), thanks to incremental learning over time. In\\naddition, IB-CNN considering both strong and weak classiﬁer losses outperforms the one with only\\nstrong-classiﬁer loss, denoted as IB-CNN-S. Note that, IB-CNN achieves a signiﬁcant improvement\\nfor recognizing AU28 (Lips suck), which has the least number of occurrences (around 1.25%\\npositive samples) in the training set, from 0.280 (CNN) and 0.144 (B-CNN) to 0.490 (IB-CNN) in\\nterms of F1 score. The performance of B-CNN is the worst for infrequent AUs due to the limited\\npositive samples in each mini-batch. In contrast, the proposed IB-CNN improves CNN learning\\nsigniﬁcantly with limited training data.\\nTable 1: Performance comparison of CNN, B-CNN, IB-CNN-S, and IB-CNN on the SEMAINE database in\\nterms of F1 and 2AFC. The format is mean±std. PPos: percentage of positive samples in the training set.\\nAUs\\nPPos\\nCNN\\nB-CNN\\nIB-CNN-S\\nIB-CNN\\nF1\\n2AFC\\nF1\\n2AFC\\nF1\\n2AFC\\nF1\\n2AFC\\nAU2\\n13.5%\\n0.314±0.065\\n0.715±0.076\\n0.241±0.073\\n0.646±0.060\\n0.414±0.016\\n0.812±0.010\\n0.410±0.024\\n0.820±0.009\\nAU12\\n17.6%\\n0.508±0.023\\n0.751±0.009\\n0.555±0.007\\n0.746±0.013\\n0.549±0.016\\n0.773±0.007\\n0.539±0.013\\n0.777±0.005\\nAU17\\n1.9%\\n0.288±0.020\\n0.767±0.014\\n0.204±0.048\\n0.719±0.036\\n0.248±0.048\\n0.767±0.011\\n0.248±0.007\\n0.777±0.012\\nAU25\\n17.7%\\n0.358±0.033\\n0.635±0.011\\n0.407±0.006\\n0.618±0.011\\n0.378±0.009\\n0.638±0.011\\n0.401±0.014\\n0.638±0.003\\nAU28\\n1.25%\\n0.280±0.111\\n0.840±0.076\\n0.144±0.092\\n0.639±0.195\\n0.483±0.069\\n0.898±0.006\\n0.490±0.078\\n0.904±0.011\\nAU45\\n19.7%\\n0.333±0.036\\n0.702±0.022\\n0.311±0.016\\n0.668±0.019\\n0.401±0.009\\n0.738±0.010\\n0.398±0.005\\n0.734±0.005\\nAVG\\n-\\n0.347±0.026\\n0.735±0.014\\n0.310±0.015\\n0.673±0.028\\n0.412±0.018\\n0.771±0.003\\n0.416±0.018\\n0.775±0.004\\n1For the CK, SEMAINE, and DISFA databases, 66 landmarks are detected [26] for face alignment and\\nwarping. For the BP4D database, the 49 landmarks provided in the database are used for face alignment.\\n2Psychological studies show that each AU activation ranges from 48 to 800 frames at 30fps [27].\\n6\\n\\nTable 2: Performance comparison of CNN, B-CNN, and IB-CNN on the DISFA database in terms of F1 score\\nand 2AFC score. The format is mean±std. PPos: percentage of positive samples in the whole database.\\nAUs\\nPPos\\nCNN\\nB-CNN\\nIB-CNN\\nF1\\n2AFC\\nF1\\n2AFC\\nF1\\n2AFC\\nAU1\\n6.71%\\n0.257±0.200\\n0.724±0.116\\n0.259±0.150\\n0.780±0.079\\n0.327±0.204\\n0.773±0.119\\nAU2\\n5.63%\\n0.346±0.226\\n0.769±0.119\\n0.333±0.197\\n0.835±0.085\\n0.394±0.219\\n0.849±0.073\\nAU4\\n18.8%\\n0.515±0.208\\n0.820±0.116\\n0.446±0.186\\n0.793±0.083\\n0.586±0.104\\n0.886±0.060\\nAU5\\n2.09%\\n0.195±0.129\\n0.780±0.154\\n0.184±0.114\\n0.749±0.279\\n0.312±0.153\\n0.887±0.076\\nAU6\\n14.9%\\n0.619±0.072\\n0.896±0.042\\n0.596±0.086\\n0.906±0.040\\n0.624±0.069\\n0.917±0.026\\nAU9\\n5.45%\\n0.340±0.131\\n0.859±0.081\\n0.331±0.115\\n0.895±0.057\\n0.385±0.137\\n0.900±0.057\\nAU12\\n23.5%\\n0.718±0.063\\n0.943±0.028\\n0.686±0.083\\n0.913±0.030\\n0.778±0.047\\n0.953±0.020\\nAU15\\n6.01%\\n0.174±0.132\\n0.586±0.174\\n0.224±0.120\\n0.753±0.091\\n0.135±0.122\\n0.511±0.226\\nAU17\\n9.88%\\n0.281±0.154\\n0.678±0.125\\n0.330±0.132\\n0.763±0.086\\n0.376±0.222\\n0.742±0.148\\nAU20\\n3.46%\\n0.134±0.113\\n0.604±0.155\\n0.184±0.101\\n0.757±0.083\\n0.126±0.069\\n0.628±0.151\\nAU25\\n35.2%\\n0.716±0.111\\n0.890±0.064\\n0.670±0.064\\n0.844±0.049\\n0.822±0.076\\n0.922±0.063\\nAU26\\n19.1%\\n0.563±0.152\\n0.810±0.073\\n0.507±0.131\\n0.797±0.054\\n0.578±0.155\\n0.876±0.039\\nAVG\\n-\\n0.405±0.055\\n0.780±0.036\\n0.398±0.059\\n0.815±0.031\\n0.457±0.067\\n0.823±0.031\\nTable 3: Performance comparison with the state-of-the-art methods on four benchmark databases in terms of\\ncommon metrics. ACC: Average classiﬁcation rate.\\nCK\\nSEMAINE\\nBP4D\\nDISFA\\nMethods\\nACC\\nMethods\\nF1\\nMethods\\nF1\\nMethods\\n2AFC\\nACC\\nAAM [29]\\n0.955\\nLGBP [11]\\n0.351\\nLGBP [11]\\n0.580\\nGabor [12]\\nN/A\\n0.857\\nGabor+DBN [30]\\n0.933\\nCNN [9]\\n0.341\\nCNN [9]\\n0.522\\nBGCS [31]\\nN/A\\n0.868\\nLBP [32]\\n0.949\\nDLA-SIFT [16]\\n0.435\\nDLA-SIFT [16]\\n0.591\\nLPQ [17]\\n0.810\\nN/A\\nML-CNN [33]\\n0.757\\n0.846\\nCNN (baseline)\\n0.937\\nCNN (baseline)\\n0.347\\nCNN (baseline)\\n0.510\\nCNN (baseline)\\n0.780\\n0.839\\nIB-CNN\\n0.951\\nIB-CNN\\n0.416\\nIB-CNN\\n0.578\\nIB-CNN\\n0.825\\n0.858\\nPerformance evaluation on the DISFA database: A 9-fold cross-validation strategy is employed\\nfor the DISFA database, where 8 subsets of 24 subjects were utilized for training and the remaining\\none subset of 3 subjects for testing. For each fold, the training-testing process was repeated 5 times.\\nThe mean and standard deviation of the F1 score and the 2AFC score are calculated from the 5 × 9\\nruns for each target AU and reported in Table 2. As shown in Table 2, the proposed IB-CNN improves\\nthe performance from 0.405 (CNN) and 0.398 (B-CNN) to 0.457 (IB-CNN) in terms of the average\\nF1 score and from 0.780 (CNN) and 0.815 (B-CNN) to 0.823 (IB-CNN) in terms of 2AFC score.\\nSimilar to the results on the SEMAINE database, the performance improvement of the infrequent\\nAUs is more impressive. AU5 (upper lid raiser) has the least number of occurrences, i.e., 2.09%\\npositive samples, in the DISFA database. The recognition performance increases from 0.195 (CNN)\\nand 0.184 (B-CNN) to 0.312 (IB-CNN) in terms of the average F1 score.\\nComparison with the State-of-the-Art methods: We further compare the proposed IB-CNN with\\nthe state-of-the-art methods, especially the CNN-based methods, evaluated on the four benchmark\\ndatabases using the metrics that are common in those papers 3. As shown in Tables 3, the perfor-\\nmance of IB-CNN is comparable with the state-of-the-art methods and more importantly, outper-\\nforms the CNN-based methods.\\n4.4\\nData Analysis\\n0.1\\n0.5\\n1\\n2\\n4\\n8\\n16\\n0.2\\n0.3\\n0.4\\n0.5\\nValue of c in function η\\nF1 Score\\nFigure 4: Recognition performance\\nversus the choice of η.\\nData analysis of the parameter η: The value of η can affect\\nthe slope of the simulated sign(·) function and consequently,\\nthe gradient and optimization process. When η is smaller than\\n0.5, the simulation is more similar to the real sign(·), but the\\nderivative is near zero for most of the input data, which can\\ncause slow convergence or divergence. An experiment was\\nconducted to analyze the inﬂuence of η = σ\\nc in Eq. 1. Speciﬁ-\\ncally, an average F1 score is calculated from all AUs in the SE-\\nMAINE database while varying the value of c. As illustrated in\\nFigure 4, the recognition performance in terms of the average\\nF1 score is robust to the choice of η when c ranges from 0.5 to\\n16. In our experiment, η is set to half of the standard deviation σ\\n2 , empirically.\\nData Analysis of the number of input neurons in the IB layer: Selecting an exact number of\\nnodes for the hidden layers remains an open question. An experiment was conducted to demonstrate\\nthat the proposed IB-CNN is insensitive to the number of input neurons. Speciﬁcally, a set of IB-\\n3Since the testing sets of the SEMAINE and BP4D database are not available, the IB-CNN is compared\\nwith the method reported on the validation sets.\\n7\\n\\nCNNs, with the number of input neurons of 64, 128, 256, 512, 1042, and 2048, were trained and\\ntested on the SEMAINE database. For each IB-CNN, the average F1 score is computed over 5 runs\\nfor each AU. As shown in Figure 5, the B-CNN and especially, the proposed IB-CNN is more robust\\nto the number of input neurons compared to the traditional CNN since a small set of neurons are\\nactive in contrast to the FC layer in the traditional CNN.\\nCNN\\nB-CNN\\nIB-CNN\\nF1 Score\\nF1 Score\\n64\\n128\\n256\\n512 1024 2048\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nAU2\\n64\\n128\\n256\\n512\\n1024 2048\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nAU12\\n64\\n128\\n256\\n512\\n1024 2048\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\nAU17\\n64\\n128\\n256\\n512\\n1024 2048\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nAU25\\n64\\n128\\n256\\n512\\n1024 2048\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nAU28\\n64\\n128\\n256\\n512 1024 2048\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nAU45\\nFigure 5: Recognition performance versus the number of input neurons in the IB layer.\\n−11\\n−10\\n−9\\n−8\\n−7\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\nLog Learning Rate\\nF1 Score\\nCNN\\nIB-CNN\\nFigure 6: Recognition performance versus\\nthe learning rate γ.\\nData analysis of learning rate γ: Another issue in CNNs\\nis the choice of the learning rate γ. The performance of\\nthe IB-CNN at different learning rates is depicted in Fig-\\nure 6 in terms of the average F1 score calculated from all\\nAUs on the SEMAINE database. Compared to the tradi-\\ntional CNN, the proposed IB-CNN is less sensitive to the\\nvalue of the learning rate.\\n5\\nConclusion and Future Work\\nIn this paper, a novel IB-CNN was proposed to integrate\\nboosting classiﬁcation into a CNN for the application of\\nAU recognition. To deal with limited positive samples in a\\nmini-batch, an incremental boosting algorithm was devel-\\noped to accumulate information from multiple batches over time. A novel loss function that accounts\\nfor errors from both the incremental strong classiﬁer and individual weak classiﬁers is proposed to\\nﬁne-tune the IB-CNN. Experimental results on four benchmark AU databases have demonstrated\\nthat the IB-CNN achieves signiﬁcant improvement over the traditional CNN, as well as the state-\\nof-the-art CNN-based methods for AU recognition. Furthermore, the IB-CNN is more effective in\\nrecognizing infrequent AUs with limited training data. The IB-CNN is a general machine learning\\nmethod and can be adapted to other learning tasks, especially those with limited training data. In the\\nfuture, we plan to extend it to multitask learning by replacing the binary classiﬁer with a multiclass\\nboosting classiﬁer.\\nAcknowledgment\\nThis work is supported by National Science Foundation under CAREER Award IIS-1149787.\\nReferences\\n[1] Ekman, P., Friesen, W.V., Hager, J.C.: Facial Action Coding System: the Manual. Research Nexus, Div.,\\nNetwork Information Research Corp., Salt Lake City, UT (2002)\\n[2] Zeng, Z., Pantic, M., Roisman, G.I., Huang, T.S.: A survey of affect recognition methods: Audio, visual,\\nand spontaneous expressions. IEEE T-PAMI 31(1) (Jan. 2009) 39–58\\n[3] Sariyanidi, E., Gunes, H., Cavallaro, A.: Automatic analysis of facial affect: A survey of registration,\\nrepresentation and recognition. IEEE T-PAMI 37(6) (June 2015) 1113–1133\\n[4] Fasel, B.: Head-pose invariant facial expression recognition using convolutional neural networks. In:\\nICMI. (2002) 529–534\\n[5] Rifai, S., Bengio, Y., Courville, A., Vincent, P., Mirza, M.: Disentangling factors of variation for facial\\nexpression recognition. In: ECCV. (2012) 808–822\\n[6] Tang, Y.: Deep learning using linear support vector machines. In: ICML. (2013)\\n8\\n\\n[7] Liu, M., Li, S., Shan, S., Wang, R., Chen, X.: Deeply learning deformable facial action parts model for\\ndynamic expression analysis. In: ACCV. (2014)\\n[8] Jung, H., Lee, S., Yim, J., Park, S., Kim, J.: Joint ﬁne-tuning in deep neural networks for facial expression\\nrecognition. In: ICCV. (2015) 2983–2991\\n[9] Gudi, A., Tasli, H.E., den Uyl, T.M., Maroulis, A.: Deep learning based FACS action unit occurrence and\\nintensity estimation. In: FG. (2015)\\n[10] Jaiswal, S., Valstar, M.F.: Deep learning the dynamic appearance and shape of facial action units. In:\\nWACV. (2016)\\n[11] Valstar, M., Girard, J., Almaev, T., McKeown, G., Mehu, M., Yin, L., Pantic, M., Cohn, J.: FERA 2015 -\\nsecond facial expression recognition and analysis challenge. FG (2015)\\n[12] Mavadati, S.M., Mahoor, M.H., Bartlett, K., Trinh, P., Cohn, J.F.: Disfa: A spontaneous facial action\\nintensity database. IEEE Trans. on Affective Computing 4(2) (2013) 151–160\\n[13] Bartlett, M.S., Littlewort, G., Frank, M.G., Lainscsek, C., Fasel, I., Movellan, J.R.: Recognizing facial\\nexpression: Machine learning and application to spontaneous behavior. In: CVPR. (2005) 568–573\\n[14] Valstar, M.F., Mehu, M., Jiang, B., Pantic, M., Scherer, K.: Meta-analysis of the ﬁrst facial expression\\nrecognition challenge. IEEE T-SMC-B 42(4) (2012) 966–979\\n[15] Baltrusaitis, T., Mahmoud, M., Robinson, P.: Cross-dataset learning and person-speciﬁc normalisation for\\nautomatic action unit detection. In: FG. Volume 6. (2015) 1–6\\n[16] Yuce, A., Gao, H., Thiran, J.: Discriminant multi-label manifold embedding for facial action unit detection.\\nIn: FG. (2015)\\n[17] Jiang, B., Martinez, B., Valstar, M.F., Pantic, M.: Decision level fusion of domain speciﬁc regions for\\nfacial action recognition. In: ICPR, IEEE (2014) 1776–1781\\n[18] Zhao, G., Pietiäinen, M.: Dynamic texture recognition using local binary patterns with an application to\\nfacial expressions. IEEE T-PAMI 29(6) (June 2007) 915–928\\n[19] Yang, P., Liu, Q., Metaxas, D.N.: Boosting encoded dynamic features for facial expression recognition.\\nPattern Recognition Letters 30(2) (Jan. 2009) 132–139\\n[20] Zafeiriou, S., Petrou, M.: Sparse representations for facial expressions recognition via L1 optimization.\\nIn: CVPR Workshops. (2010) 32–39\\n[21] Liu, P., Han, S., Meng, Z., Tong, Y.: Facial expression recognition via a boosted deep belief network. In:\\nCVPR. (2014)\\n[22] Nagi, J., Di Caro, G.A., Giusti, A., Nagi, F., Gambardella, L.M.: Convolutional neural support vector\\nmachines: hybrid visual pattern classiﬁers for multi-robot systems. In: ICMLA. (2012) 27–32\\n[23] Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Improving neural networks\\nby preventing co-adaptation of feature detectors. arXiv preprint (2012)\\n[24] Medera, D., Babinec, S.: Incremental learning of convolutional neural networks. In: IJCCI. (2009) 547–\\n550\\n[25] Kanade, T., Cohn, J.F., Tian, Y.: Comprehensive database for facial expression analysis. In: FG. (2000)\\n46–53\\n[26] Asthana, A., Zafeiriou, S., Cheng, S., Pantic, M.: Robust discriminative response map ﬁtting with con-\\nstrained local models. In: CVPR. (2013) 3444–3451\\n[27] Sayette, M.A., Cohn, J.F., Wertz, J.M., Perrott, M.A., Parrott, D.J.: A psychometric evaluation of the\\nfacial action coding system for assessing spontaneous expression. J. Nonverbal Behavior 25(3) (2001)\\n167–185\\n[28] Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T.: Caffe:\\nConvolutional architecture for fast feature embedding. In: ACM MM, ACM (2014) 675–678\\n[29] Lucey, S., Ashraf, A.B., Cohn, J.: Investigating spontaneous facial action recognition through AAM rep-\\nresentations of the face. In Kurihara, K., ed.: Face Recognition Book. Pro Literatur Verlag, Mammendorf,\\nGermany (April 2007)\\n[30] Tong, Y., Liao, W., Ji, Q.: Facial action unit recognition by exploiting their dynamic and semantic rela-\\ntionships. IEEE T-PAMI 29(10) (October 2007) 1683–1699\\n[31] Song, Y., McDuff, D., Vasisht, D., Kapoor, A.: Exploiting sparsity and co-occurrence structure for action\\nunit recognition. In: FG. (2015)\\n[32] Han, S., Meng, Z., Liu, P., Tong, Y.: Facial grid transformation: A novel face registration approach for\\nimproving facial action unit recognition. In: ICIP. (2014)\\n[33] Ghosh, S., Laksana, E., Scherer, S., Morency, L.: A multi-label convolutional neural network approach to\\ncross-domain action unit detection. ACII (2015)\\n9\\n',\n",
       "  'metadata': {'authors': ['Shizhong Han',\n",
       "    'Zibo Meng',\n",
       "    'Ahmed Shehab Khan',\n",
       "    'Yan Tong'],\n",
       "   'published': datetime.datetime(2017, 7, 17, 21, 4, 13, tzinfo=datetime.timezone.utc)}},\n",
       " {'title': 'DeepVisage: Making face recognition simple yet with powerful generalization skills',\n",
       "  'text': 'DeepVisage: Making face recognition simple yet with powerful generalization\\nskills\\nAbul Hasnat1, Julien Bohn´e2, Jonathan Milgram2, St´ephane Gentric2, and Liming Chen1\\n1Laboratoire LIRIS, ´Ecole centrale de Lyon, 69134 Ecully, France.\\n2Safran Identity & Security, 92130 Issy-les-Moulineaux, France.\\nmd-abul.hasnat@ec-lyon.fr, {julien.bohne, stephane.gentric, jonathan.milgram}@safrangroup.com,\\nliming.chen@ec-lyon.fr\\nAbstract\\nFace recognition (FR) methods report signiﬁcant perfor-\\nmance by adopting the convolutional neural network (CNN)\\nbased learning methods. Although CNNs are mostly trained\\nby optimizing the softmax loss, the recent trend shows an\\nimprovement of accuracy with different strategies, such as\\ntask-speciﬁc CNN learning with different loss functions,\\nﬁne-tuning on target dataset, metric learning and concate-\\nnating features from multiple CNNs. Incorporating these\\ntasks obviously requires additional efforts.\\nMoreover, it\\ndemotivates the discovery of efﬁcient CNN models for FR\\nwhich are trained only with identity labels. We focus on this\\nfact and propose an easily trainable and single CNN based\\nFR method. Our CNN model exploits the residual learn-\\ning framework. Additionally, it uses normalized features to\\ncompute the loss. Our extensive experiments show excel-\\nlent generalization on different datasets. We obtain very\\ncompetitive and state-of-the-art results on the LFW, IJB-A,\\nYouTube faces and CACD datasets.\\n1. Introduction\\nFace recognition (FR) is one of the most demanding\\ncomputer vision tasks, due to its practical use in numerous\\napplications, such as biometric, surveillance and human-\\nmachine interaction. The state-of-the-art FR methods [34,\\n29, 31, 24, 20] surpassed human performance (97.53%) and\\nachieved signiﬁcant accuracy on the standard labeled faces\\nin the wild (LFW) [14] benchmark. These remarkable re-\\nsults are achieved by training the deep convolutional neural\\nnetwork (CNN) [10] with large databases [11, 24, 44, 2].\\nThe facial image databases mostly provide the identity\\nlabels. These labels allow the CNN models to be easily\\ntrained with the softmax loss. FR methods generally use\\nthe trained CNN model to extract facial features and then\\nperform veriﬁcation by computing distance or recognition\\nwith a classiﬁer. However, from our extensive study (see\\nSect. 2), we observe that recent methods include different\\nadditional strategies to obtain better performance, such as:\\n1. train CNN with different loss functions [29, 31]: re-\\nquires carefully preparing the image pairs/triplets by\\nmaintaining certain constrains [29], because arbitrary\\npairs/triplets do not contribute to the training.\\nOn-\\nline triplet generation requires a larger batch size (e.g.,\\n[29] used 1.8K images in a mini-batch with 40 im-\\nages/identity), which is excessive for a limited re-\\nsource machine.\\nOn the other hand, using ofﬂine\\ntriplets can be critical as many of them will be useless\\nwhile training progresses. The joint optimization [31]\\nwith Softmax and Contrastive losses not only requires\\nspeciﬁc training data (with identity and pair labels) but\\nalso complicates the training procedure.\\n2. ﬁne-tune CNN: requires training on each target dataset,\\nwhich restricts the ability to generalize.\\n3. metric learning [28, 9]: requires particular form of\\ntraining data (e.g., triplets). Moreover, it does not al-\\nways guarantee to enhance performance [37].\\n4. concatenating features from multiple CNNs [31, 20]:\\nrequires additional training data of different forms and\\ntrain CNNs for each form. Besides, it is necessary to\\nexplore the particular modalities that can contribute to\\nenhance performance.\\nThe use of the above strategies requires signiﬁcant ef-\\nforts in terms of data preparation or selection and comput-\\ning resources. On the other hand, recent results on the Im-\\nageNet challenge [26] indicate that deeper CNNs enhance\\n1\\narXiv:1703.08388v2  [cs.CV]  7 Apr 2017\\n\\nperformance of different computer vision tasks. These ob-\\nservations raise the following question - can we achieve\\nstate-of-the-art results with a single CNN model which is\\ntrained only once with the identity labels? Our research is\\nmotivated by this question and we aim to address it by de-\\nveloping a simple yet robust single-CNN based FR method.\\nMoreover, we want that our once-trained single CNN based\\nFR method generalizes well across different datasets.\\nIn this research, our primary objective is to discover an\\nefﬁcient CNN architecture. We follow the recent ﬁndings,\\nwhich suggest that deeper CNNs perform better on a num-\\nber of computer vision tasks [13, 10]. We construct a deep\\nCNN model with 27 convolutional and 1 fully connected\\n(FC) layers, which incorporates the residual learning frame-\\nwork [13]. Moreover, we aim to ﬁnd an efﬁcient way to\\ntrain our CNN only with the identity labels. Recently, [39]\\nachieves high FR performance with a CNN trained from the\\nidentity labels. However, they perform joint optimization\\nusing the softmax and center loss [39] (CL). CL improves\\nthe features discrimination among different classes. It fol-\\nlows the principle that, features learned from a deep CNN\\nshould minimize the intra-class distances. Interestingly, we\\nobserve (see Fig 3) that an equivalent representation can be\\nachieved by normalizing the CNN features before comput-\\ning the loss. Therefore, we train our CNN using the softmax\\nloss with the normalized features.\\nWith our single CNN model, ﬁrst we evaluate on the\\nLFW [14] benchmark and observe that it obtains 99.62%\\naccuracy. In order to demonstrate its effectiveness, we eval-\\nuated it on different challenging face veriﬁcation tasks, such\\nas face templates matching on the IJB-A [16] dataset, video\\nfaces matching on the YouTube Faces [40] (YTF) dataset\\nand cross age face matching on the CACD [3] dataset. Our\\nmethod achieves 82.4% TAR@FAR=0.001 on IJB-A [16],\\n96.24% accuracy on YTF [40] and 99.13% accuracy on\\nCACD [3]. These results indicate that our method achieves\\nvery competitive and state-of-the-art results. Moreover, it\\ngeneralizes very well across different datasets.\\nWe summarize our contributions as follows: (a) conduct\\nextensive study and provide (Sec 2) a review and method-\\nological comparison of the state-of-the-art methods; (b)\\npropose (Sect. 3) an efﬁcient single CNN based FR method;\\n(c) conduct (Sect.\\n4) extensive experiments on different\\ndatasets, which demonstrate that our method has excellent\\ngeneralization ability; and (d) perform (Sect. 4.3) an in-\\ndepth analysis to identify the inﬂuences of different aspects.\\nIn the remaining part of this paper, ﬁrst we study and an-\\nalyze the state-of-the-art FR methods in Section 2, describe\\nour proposed method in Section 3, present experimental re-\\nsults, perform analysis of our method and discuss them in\\nSection 4 and ﬁnally draw conclusions in Section 5.\\n2. Related work, state-of-the-art FR methods\\nFace recognition (FR) in unconstrained environment at-\\ntracts signiﬁcant interest from the community.\\nRecent\\nmethods exploited deep CNN models and achieved remark-\\nable results on the LFW [14] benchmark. Besides, numer-\\nous methods have been evaluated on the IJB-A [16] dataset.\\nWe study1 and analyze these methods based on several key\\naspects: (a) details of the CNN model; (b) loss functions\\nused; (c) incorporation of additional learning strategy; (d)\\nnumber of CNNs and (e) the training database used.\\nRecent methods tend to learn CNN based features us-\\ning a deep architecture (e.g., 10 or more layers). This is\\ninspired from the extraordinary success on the ImageNet\\n[26] challenge by famous CNN architectures [10], such\\nas AlexNet, VGGNet, GoogleNet, ResNet, etc. The FR\\nmethods commonly use these architectures as their base-\\nline model (directly or slightly modiﬁed).\\nFor example,\\nAlexNet is used by [27, 28, 21, 25, 1, 22, 29], VGGNet\\nis used by [24, 8, 23, 1, 22, 9, 33] and GoogleNet is used\\nby [42, 29]. CASIA-Webface [44] proposed a simpler CNN\\nmodel, which is used by [37, 5, 9]. Several methods, such\\nas [32, 34, 35, 39, 31] use a model with lower depth but in-\\ncrease its complexity with locally connected convolutional\\nlayers. Besides, [46] use 4 parallel 10 layers CNNs to learn\\nfeatures from different facial regions. We follow the ResNet\\n[13] based deep CNN model.\\nFR methods often train multiple CNNs and accumulate\\nfeatures from all of them to construct the ﬁnal facial de-\\nscriptors.\\nIt provides an additional boost to the perfor-\\nmance. Different types of inputs are used to train these\\nmultiple CNNs: (a) [32, 31, 33, 37, 9, 20] used image-crops\\nfocused on certain facial regions (eyes, nose, lips, etc.); (b)\\n[9, 1, 22, 34] used different modality of input images, such\\nas 2D, 3D, frontalized and synthesized faces at different\\nposes and (c) [35, 20] used different training databases with\\nvarying number of images. We do not follow this approach\\nand train only one CNN.\\nThe CNN model parameters are learned by optimizing\\nloss functions, which are deﬁned based on the given task\\n(e.g., classiﬁcation, regression) and the available informa-\\ntion (e.g., class labels, price). The softmax loss [10] is a\\ncommon choice for classiﬁcation tasks.\\nIt is often used\\nby the FR methods to create good face representation by\\ntraining the CNN as an identity classiﬁer. It requires only\\nthe identity labels. The contrastive loss [10, 7] is used by\\n[32, 34, 33, 31, 42] for face veriﬁcation and requires face\\nimage pairs and similarity labels. The triplet loss [29] is\\nused by [29, 24, 8, 20] for face veriﬁcation and requires the\\nface triplets. Recently the center loss [39] is proposed to en-\\nhance feature discrimination, which uses the identity labels.\\n1We consider only the CNN based methods. For the others, we refer\\nreaders to the recently published survey [17] for LFW and [16] for IJB-A.\\n\\nWe use the softmax loss.\\nSeveral methods use multiple loss functions and train\\nCNN using joint optimization [32, 33, 31, 39, 25]. The other\\nway is to use them sequentially [34, 24, 8, 20, 42], i.e., ﬁrst\\ntrain with the softmax and then train with the other loss.\\nWe observe that using multiple loss functions complicates\\nthe training data preparation task and the CNN training pro-\\ncedure. Therefore, we avoid this type of strategies.\\nFine-tuning the CNN parameters is a particular form\\nof transfer learning. It is commonly employed by several\\nmethods [37, 5, 27] on the IJB-A [16] dataset. It reﬁnes\\nthe CNN parameters from a previously learned model us-\\ning a target speciﬁc training dataset. Several methods do\\nnot directly use the raw CNN features but employ an addi-\\ntional learning strategy. The metric/distance learning strat-\\negy based on the Joint Bayesian method [4] is a popular one\\nand used by [32, 44, 37, 5, 33, 31, 9]. Recently, two differ-\\nent strategies [28, 28] have been proposed to learn feature\\nembedding using face triplets. Another strategy, called tem-\\nplate adaptation [8], exploits an additional SVM classiﬁer.\\nApart from these, principal component analysis (PCA) is\\nused by several methods [23, 1, 22] to learn a dataset spe-\\nciﬁc projection matrix. [42] learns an aggregation module\\nto compute scores among two videos. The above methods\\noften need training data from the target datasets. Moreover,\\nthey [27, 28] may need to carefully prepare the training data,\\ne.g., triplets. We do not need any such learning strategies.\\nThe use of a large facial training dataset is impor-\\ntant to achieve high FR accuracy [29, 46]. [46] provided\\nan in-depth analysis and demonstrated the effect of the\\ndataset size and the number of identities for FR. Follow-\\ning the high demand of a large FR dataset, several pub-\\nlicly available datasets have been released recently. Among\\nthem, CASIA-WebFace [44] is used by numerous methods\\n[39, 27, 28, 21, 25, 44, 37, 5, 9, 41, 23, 1, 22]. Several re-\\nsearches [23, 1, 22] enlarge it by synthesizing facial images\\nwith different shapes and poses based on the 3D face mod-\\nels. Recently, the MSCeleb [11] dataset has been publicly\\nreleased. It contains the largest collection of facial images\\nand identities. We exploit it to develop our FR method.\\n3. Proposed Method\\nOur FR method, called DeepVisage, consists in pre-\\nprocessing face image, learning CNN based facial fea-\\ntures and computing similarity. Following the recent trend\\n[34, 29, 31, 24, 44], we exploit the CNN as the core com-\\nponent. Our deep CNN model follows the residual learning\\nframework [13]. Moreover, it intelligently exploits feature\\nnormalization, which is a crucial step, see Sect. 4.3. Our\\npre-processing stage consists in the detection of the face\\nand facial landmarks, which are used to create a normalized\\nface image. We compute the cosine similarity among the\\nfeatures of a pair of faces as the veriﬁcation score. Below,\\nwe describe these elements.\\n3.1. Building blocks and deep CNN architecture\\nConvolutional networks:\\nWe begin with the basic ideas\\nof CNN [18]:\\n(a) local receptive ﬁelds with identical\\nweights via the convolution operation and (b) spatial sub-\\nsampling via the pooling operation. At a particular layer l,\\nthe convolution of the input f Op,l−1\\nx,y\\nto obtain the kth output\\nfeature map f C,l\\nx,y,k, can be expressed as:\\nf C,l\\nx,y,k = wl\\nk\\nT f Op,l−1\\nx,y\\n+ bl\\nk\\n(1)\\nwhere, wl\\nk and bl\\nk are the shared weights and bias. C de-\\nnotes convolution and Op (for l > 1) denotes various tasks,\\nsuch as convolution, sub-sampling or activation. For l = 1,\\nOp represents the input image. Sub-sampling or pooling\\nperforms a simple local operation, such as computing the\\naverage or maximum value in a local spatial neighborhood\\nfollowed by reducing spatial resolution.\\nWe apply max\\npooling for our CNN, which has the following form:\\nf P,l\\nx,y,k =\\nmax\\n(m,n)∈Nx,y f Op,l−1\\nm,n,k\\n(2)\\nwhere, Nx,y denotes the local spatial neighborhood of\\n(x, y) coordinate and P denotes the pooling operation.\\nIn order to ensure non-linearity of the network, the\\nfeature maps are passed through a non-linear activation\\nfunction, e.g., the Rectiﬁed Linear Unit (ReLU) [10, 12]:\\nf l\\nx,y,k = max(f l−1\\nx,y,k, 0). We apply the Parametric Recti-\\nﬁed Linear Unit (PReLU) [12] as the activation function,\\nwhich has the following form:\\nf A,l\\nx,y,k = max(f Op,l−1\\nx,y,k\\n, 0) + λkmin(f Op,l−1\\nx,y,k\\n, 0)\\n(3)\\nwhere, λk is a trainable parameter to control the slope of the\\nlinear function for the negative input values and A denotes\\nactivation operation.\\nAt the basic level, a CNN is constructed by stacking\\nseries of convolution, activation and pooling layers, see\\nLeNet-5 [18] for an example. Often a layer with full con-\\nnections is placed at the end of the stacked layers, called\\nthe fully connected (FC) layer. It takes all points (neurons)\\nfrom the previous layer as input and connects it to all points\\n(neurons) of the output layer.\\nResidual learning framework [13]:\\nA recent trend [10]\\non the ImageNet [26] challenge shows that deeper CNNs\\nachieve better results. However, it increases the model com-\\nplexity, which makes it harder to optimize the loss of the\\nCNN model. Besides, they may generate higher training er-\\nror than a shallower CNN [13]. The residual learning frame-\\nwork [13] provides a solution to these problems.\\nFor a stack of a few layers, residual learning ﬁts a map-\\nping F(f) := H(f) −f instead of ﬁtting the underlying\\n\\nmapping H(f). Therefore, the original mapping is formu-\\nlated as F(f) + f, which means directly adding the in-\\nput feature map f with the output of the stacked layers\\nF(f). This idea can be easily implemented with the notion\\nof shortcut connection. Formally, the output of a residual\\nblock R can be expressed as:\\nf R,l\\nx,y,k = f Op,l−q\\nx,y\\n+ F(f Op,l−q\\nx,y\\n, {Wk})\\n(4)\\nwhere, f Op,l−q\\nx,y\\nrepresents the input feature map, F(.) is the\\nresidual mapping to be learned, Wk is the parameters of the\\nkth residual block and q is the total number of stacked layers\\nwithin the residual block. The ﬂexible form of the residual\\nfunction F(.) allows to stack multiple layers with different\\ntypes of operations, such as convolution, pooling, activation\\netc. All of the residual blocks in our CNN consist of two\\nconvolution layers with different numbers of neurons. Each\\nconvolution is followed by a PReLU activation.\\nLoss function:\\nDeep CNNs are trained by optimizing loss\\nfunction. We use the softmax loss, which is widely used for\\nclassiﬁcation:\\nLSoftmax = −\\nN\\nX\\ni=1\\nlog\\newT\\nyifi+byi\\nPK\\nj=1 ewT\\nj fi+bj\\n(5)\\nwhere, fi and yi are the features and true class label of the\\nith image. wj and bj denote the weights and bias of the jth\\nclass. N and K denote the number of training samples and\\nthe number of classes.\\nFeature normalization (FN):\\nIt is often used as a neces-\\nsary step in many learning algorithms. It ensures that all\\nof the features have equal contribution to the cost function\\n[36]. With deep CNNs, we cannot guarantee this by only\\nnormalizing the input image pixels, because the scale of fea-\\ntures (from the ﬁnal FC layer) may change due to a series\\nof operations at different layers. Therefore, to avoid the in-\\nﬂuence of un-normalized features during cost computation,\\nwe provide normalized features f Nr\\ni\\nto the softmax loss as:\\nf Nr = f Op−µ\\n√\\nσ2 , where µ and σ2 are the mean and variance.\\nDuring training, we apply normalization by computing\\nµ and σ from the samples of each mini-batch. Moreover,\\nwe maintain the moving average of µ and σ and use them\\nto normalize the test samples. Note that, this is a speciﬁc\\ncase of the popular batch normalization (BN) technique [15]\\nwith scale γ = 1 and shift β = 0.\\nProposed CNN architecture:\\nOur CNN model consists\\nof 27 convolution (Conv), 4 pooling (Pool) and 1 fully con-\\nnected (FC) layers. Each convolution uses a 3 × 3 kernel\\nand is followed by a PReLU activation function. The CNN\\nprogresses from the lower to higher depth by decreasing the\\nFigure 1: Illustration of the proposed CNN architecture. CoPr indicates\\nconvolution followed by the PReLU activation function. ResBl is a resid-\\nual block which computes output = input + CoPr(CoPr(input)).\\n# Replication indicates how many times the same block is sequentially\\nreplicated in the CNN model. # Filts denotes the number of feature maps.\\nFN denotes feature normalization.\\nspatial resolution using a 2 × 2 max Pool layer while gradu-\\nally increasing the number of feature maps from 32 to 512.\\nWe use a FC layer of 512 neurons after the last Conv layer.\\nWe normalize (see FN above) the output of this FC layer\\nand consider it as the desired feature representation of the\\ninput image. Finally, we use the softmax layer to compute\\nthe loss and optimize it during training. Our CNN model\\nincorporates the residual learning framework [13], see Fig.\\n1 for the details. Overall, it comprises 40.5M parameters.\\n3.2. Image pre-processing and face veriﬁcation\\nPre-processing:\\nWe maintain the same form of the 2D\\nface image during training and testing. Our pre-processing\\nsteps are: (a) detect2 face and landmarks using the MTCNN\\n[45] detector; (b) normalize the face image by applying a\\n2D similarity transformation. The transformation parame-\\nters are computed from the location of the detected land-\\nmarks on the image and pre-set coordinates in a 112×96\\nimage frame; and (c) convert to grayscale.\\nFace veriﬁcation:\\nWe verify a pair of face images [14],\\ntemplates [16] (contains multiple images and video frames)\\nand videos [40] (given as frames) using the following steps:\\n1. pre-process: apply the pre-processing3 stage described\\nin the previous paragraph.\\n2. extract facial feature/representation:\\nwe use the\\ntrained CNN model to extract the facial feature de-\\nscriptor. For an image i, we obtain its descriptor fi\\nby taking element-wise maximum of the features from\\nits original fi,o and horizontally ﬂipped version fi,f.\\nIn order to perform veriﬁcation based on template [16]\\nand video [40], we obtain the descriptor for an identity\\nby taking element-wise average of the features from all\\nof the images/frames.\\n3. compute veriﬁcation score: for a given pair of facial\\nfeatures, we compute the cosine similarity as the veri-\\n2In case of multiple faces, we take the face closer to the image center.\\n3If the landmarks detector fails we keep the face image by cropping it\\nbased on the given/detected bounding box.\\n\\nﬁcation score. We compare this score to a threshold to\\ndecide whether two images belong to the same person.\\n4. Experiments, Results and Discussion\\nOur experiments consist of ﬁrst training the CNN model\\nand then use it to extract facial features and perform dif-\\nferent types (single-image [14, 3], multi-image or video\\n[16, 40]) of face veriﬁcation. In order to verify the effec-\\ntiveness, we experiment on several datasets, namely LFW\\n[14], IJB-A [16], YTF [40] and CACD [3].\\n4.1. CNN Training\\nWe collect the training images from the cleaned4 ver-\\nsion of the MS-Celeb-1M [11] database, which consists of\\n4.47M images of 62.5K identities. We train our CNN model\\nusing only the identity label of each image. We use 95%\\nimages (4.2M images) for training and 5% images (232K\\nimages) for monitoring and evaluating the loss and accu-\\nracy. We train our CNN using the stochastic gradient de-\\nscent method and momentum set to 0.9. Moreover, we apply\\nL2 regularization with the weight decay set to 5e−4. We be-\\ngin the CNN training with a learning rate 0.1 for 2 epochs.\\nThen we decrease it after each epoch by a factor 10. We\\nstop the training after 5 epochs. We use 120 images in each\\nmini-batch. During training, we apply data augmentation\\nby horizontally ﬂipping the images. Note that, during evalu-\\nation on a particular dataset, we do not apply any additional\\nCNN training or ﬁne-tuning and dimension reduction.\\n4.2. Results and Evaluation\\nNow we evaluate our proposed FR method, called Deep-\\nVisage, on the most commonly used and challenging facial\\nimage datasets based on their speciﬁed protocols.\\nLabeled Faces in the Wild (LFW) [14]:\\nLFW is one of\\nthe most popular and challenging databases for evaluating\\nunconstrained FR methods. It consists of 13,233 images of\\n5,759 identities. It has different evaluation protocols. We\\nfollow the unrestricted-labeled-outside-data protocol based\\non the recent trend [17]. The FR task requires verifying\\n6000 image pairs in 10 folds and report the accuracy. These\\npairs are equally divided into genuine and impostor pairs\\nand comprises 7.7K images of 4,281 identities.\\nTable 1 provides the results of our method along with the\\nother state-of-the-art methods. We observe that, our method\\nachieves signiﬁcant accuracy (99.62%) and among the top\\nperformers, despite the fact that: (a) we use single CNN,\\nwhereas Baidu [20] used 10 CNNs to obtain 99.77% and (b)\\nwe train CNN with comparatively much less amount of data\\n4We take the list of 5.05M faces provided by [41] and keep non-\\noverlapping (with test set) identities which has at least 30 images after\\nsuccessful landmarks detection.\\nand identities, whereas FaceNet [29] used 200M images of\\n8M identities to obtain 99.63%.\\nTable 1: Comparison of the state-of-the-art methods evaluated on the\\nLFW benchmark [14].\\nFR method\\n# of\\nCNNs\\nDataset\\nInfo\\nAcc\\n%\\nDeepVisage (proposed)\\n1\\n4.48M, 62K\\n99.62\\nBaidu [20]\\n10\\n1.2M, 1.8K\\n99.77\\nBaidu [20]\\n1\\n1.2M, 1.8K\\n99.13\\nFaceNet [29]\\n1\\n200M, 8M\\n99.63\\nSparse ConvNet [33]\\n25\\n0.29M, 12K\\n99.55\\nDeepID3 [31]\\n25\\n0.29M, 12K\\n99.53\\nMegvii [46]\\n4\\n5M, 0.2M\\n99.50\\nLF-CNNs [38]\\n25\\n0.7M, 17.2K\\n99.50\\nDeepID2+ [32]\\n25\\n0.29M, 12K\\n99.47\\nCenter Loss [39]\\n1\\n0.7M, 17.2K\\n99.28\\nMM-DFR [9]\\n8\\n0.49M, 10.57K\\n99.02\\nVGG Face [24]\\n1\\n2.6M, 2.6K\\n98.95\\nMFM-CNN [41]\\n1\\n5.1M, 79K\\n98.80\\nVIPLFaceNet [21]\\n1\\n0.49M, 10.57K\\n98.60\\nWebscale [35]\\n4\\n4.5M, 55K\\n98.37\\nAAL [43]\\n1\\n0.49M, 10.57K\\n98.30\\nFSS [37]\\n9\\n0.49M, 10.57K\\n98.20\\nFace-Aug-Pose-Syn [23]\\n1\\n2.4M, 10.57K\\n98.06\\nCASIA-Webface [44]\\n1\\n0.49M, 10.57K\\n97.73\\nUnconstrained FV [5]\\n1\\n0.49M, 10.5K\\n97.45\\nDeepface [34]\\n3\\n4.4M, 4K\\n97.35\\nThe results in the Table 1 indicates saturation, because\\nall of the methods achieve close to or more than human\\nperformance (97.53%). Besides, it is argued that match-\\ning only 6K pairs is insufﬁcient to justify a method w.r.t.\\nthe real world FR scenario [19].\\nWe address these is-\\nsues by two ways: (a) employ more challenging evalua-\\ntion metrics and (b) evaluate with the other challenging\\ndatasets.\\nTo this aim, ﬁrst we follow the BLUFR LFW\\nprotocol [19] and measure the true accept rate (TAR) at\\na low false accept rate (FAR). BLUFR [19] protocol ex-\\nploits all images of the LFW dataset and evaluates meth-\\nods based on 10 trials experiments. Each trial computes\\n47M pair-matching scores (157K positives, 46.9M nega-\\ntives), which is signiﬁcantly higher than the 6K scores used\\nin the standard protocol.\\nWithin this protocol, we com-\\npute the veriﬁcation rate (VR) at FAR=0.1% and compare\\nwith the methods which reported results5 in this proto-\\ncol. We observe that: DeepVisage (proposed) (98.65) >\\nCenterLoss6 [39] (92.97%) > FSS [37] (89.8%) >\\nCASIA [44] (80.26%) , i.e., our method obtains the best\\nresults published so far. Therefore, this result together with\\nthe Table 1 conﬁrm the remarkable performance of Deep-\\n5We do not include results from Baidu [20] (VR@FAR: 99.11% for\\nsingle CNN and 99.41% for 10-CNNs ensembles). The reason is that, we\\nare not sure if they compute results based on the BLUFR protocol [19] or\\nbased on the 6K pairs. Note that, we obtain 99.7% on VR@FAR=0.1%\\nusing the 6K pair-matching scores of the standard protocol.\\n6Results computed from the features publicly provided by the authors.\\n\\nTable 2: Comparison of the state-of-the-art methods evaluated on the\\nIJB-A benchmark [16]. ‘-’ indicates the information for the entry is un-\\navailable. Methods which incorporates external training (ExTr) or CNN\\nﬁne-tuning (FT) with IJB-A training data are separated with a horizontal\\nline. VGG-Face result was provided by [27]. T@F denotes the True Accept\\nRate at a ﬁxed False Accept Rate (TAR@FAR).\\nFR method\\nExTr\\nFT\\nT@F\\n0.01\\nT@F\\n0.001\\nDeepVisage (proposed)\\nN\\nN\\n0.887\\n0.824\\nVGG Face [24]\\nN\\nN\\n0.805\\n0.604\\nFace-Aug-Pose-Syn [23]\\nN\\nN\\n0.886\\n0.725\\nDeep Multipose [1]\\nN\\nN\\n0.787\\n-\\nPose aware FR [22]\\nN\\nN\\n0.826\\n0.652\\nTPE [28]\\nN\\nN\\n0.871\\n0.766\\nAll-In-One [25]\\nN\\nN\\n0.893\\n0.787\\nAll-In-One [25] + TPE\\nY\\nN\\n0.922\\n0.823\\nSparse ConvNet [33]\\nY\\nN\\n0.726\\n0.460\\nFSS [37]\\nN\\nY\\n0.729\\n0.510\\nTPE [28]\\nY\\nN\\n0.900\\n0.813\\nUnconstrained FV [5]\\nY\\nY\\n0.838\\n-\\nTSE [27]\\nY\\nY\\n0.790\\n0.590\\nNAN [42]\\nY\\nN\\n0.941\\n0.881\\nTA [8]\\nY\\nN\\n0.939\\n0.836\\nEnd-To-End [6]\\nN\\nY\\n0.787\\n-\\nVisage on the LFW database. Next, we justify our method\\nby evaluating it on the challenging IJB-A [16] dataset.\\nIARPA Janus Benchmark A (IJB-A) [16]:\\nThe recently\\nproposed IJB-A database aims at raising the difﬁculty of\\nFR by incorporating more variations in pose, illumination,\\nexpression, resolution and occlusion. It consists of 5,712\\nimages and 2,085 videos of 500 identities. The FR task\\ncompares two templates. A template is a set of images and\\nvideo-frames. The evaluation protocol requires computing\\nthe true accept rate (TAR) at a ﬁxed false accept rate (FAR)\\nwith various values, e.g., 0.01 and 0.001.\\nTable 2 presents our results along with the other state-of-\\nthe-art methods. We separate the results (with a horizontal\\nline) to distinguish two categories: (1) methods only us-\\ning a pre-trained CNN; our method belongs to this category\\nand (2) methods use additional learning, such as CNN ﬁne-\\ntuning and metric learning. From the comparison among\\nthe 1st category of methods, we observe that, our method\\nprovides the best result for FAR at 0.001% and competitive\\n(second best) at 0.01%. By comparing it to the 2nd category\\nwe observe that, it is also very competitive and provide bet-\\nter results than numerous methods from this category. Be-\\nsides, similar to [25, 28], it is possible to exploit our CNN\\nfeatures and further improve the ﬁnal results with external\\nlearning, such as TA [8], NAN [42] and TPE [28].\\nYouTube Faces [40] (YTF):\\nThe YTF dataset is a widely\\nused FR dataset of unconstrained videos.\\nIt consists of\\n3,425 videos of 1,595 identities. YTF evaluation requires\\nmatching 5000 video pairs in 10 folds and report average\\naccuracy. Each fold consists of 500 video pairs and en-\\nsures subject-mutually exclusive property. We follow the\\nrestricted protocol of YTF, i.e., access to only the similar-\\nity information. We report our result in Table 3, along with\\nthe state-of-the-art methods. Results show that our method\\nprovides the best accuracy (96.24%).\\nTable 3 also provides the results (separated with a hori-\\nzontal line) from unrestricted protocol, i.e., access to simi-\\nlarity and identity information of the test data. We observe\\nthat our method is very competitive to the best accuracy, al-\\nthough it follows the restricted protocol. The VGG Face\\n[24] provides results with both protocols and shows that\\naccuracy increases signiﬁcantly (from restricted-91.6% to\\nunrestricted-97.3%) when they learn their CNN feature em-\\nbedding using the YTF training data. Based on this observa-\\ntion, we can predict that our result (96.24%) can be further\\nenhanced by training or ﬁne tuning with the YTF data.\\nTable 3: Comparison of the state-of-the-art methods evaluated on the\\nYoutube Face [40]. Ad.Tr. denotes additional training is used.\\nFR method\\nAd.Tr.\\nAccuracy (%)\\nDeepVisage (proposed)\\nN\\n96.24\\nVGG Face [24]\\nN\\n91.60\\nSparse ConvNet [33]\\nN\\n93.50\\nFaceNet [29]\\nN\\n95.18\\nDeepID2+ [32]\\nN\\n93.20\\nCenter Loss [39]\\nN\\n94.90\\nMFM-CNN [41]\\nN\\n93.40\\nCASIA-Webface [44]\\nY\\n92.24\\nDeepface [34]\\nY\\n91.40\\nVGG Face [24]\\nY\\n97.30\\nNAN [42]\\nY\\n95.72\\nCross-Age Celebrity Dataset (CACD) [3]:\\nCACD is a re-\\ncently released dataset, which aims to ensure large varia-\\ntions of the ages in the wild. It consists of 163,446 im-\\nages of 2000 identities with the age range from 16 to 62.\\nCACD evaluation requires verifying 4000 image pairs in\\nten folds and report average accuracy. Table 4 reports the\\nresults of DeepVisage along with the state-of-the-art meth-\\nods. It shows that our method provides the best accuracy.\\nMoreover, it is better than LF-CNN [38], which is a recent\\nmethod specialized on age invariant face recognition.\\nThe evaluations of DeepVisage (proposed method)\\nacross different challenging datasets prove that it not only\\nachieves signiﬁcant performance but also generalizes very\\nwell. It overcomes several of the difﬁculties which make\\nunconstrained FR a challenging task.\\n4.3. Analysis and Discussion\\nWe perform further analysis to highlight the inﬂuences\\nof several aspects, such as: (a) training datasets; (b) CNN\\n\\nTable 4: Comparison of the state-of-the-art methods evaluated on the\\nCACD [3] dataset. VGG [24] result is obtained from [41].\\nFR method\\nAccuracy (%)\\nDeepVisage (proposed)\\n99.13\\nLF-CNNs [38]\\n98.50\\nMFM-CNN [41]\\n97.95\\nVGG Face [24]\\n96.00\\nCARC [3]\\n87.60\\nHuman, Avg.\\n85.70\\nHuman, Voting [3]\\n94.20\\nmodels and depth; (c) normalization and (d) activation func-\\ntions. Therefore, we modify and train our CNN model and\\nobserve the accuracy and TAR@FAR=0.01 on LFW. Table\\n5 presents the results.\\nFirst, we study the inﬂuence of training the proposed\\nCNN with different datasets. It helps us to understand the\\ncapacity of the CNN to learn facial representation and iden-\\ntify the requirements to achieve better performance. The top\\npart of Table 5 presents the analysis w.r.t. different datasets,\\nfrom which we observe that: (a) CNN performance in-\\ncreased by training with larger number of images as well\\nas identities, the best results are obtained with the largest\\ndataset, i.e., MSCeleb [11]; (b) synthesized images help to\\nenhance performance, we see this from the pose augmented\\nCASIA [44, 23] dataset; (c) a dataset with more variations\\nper identity helps even with a relatively lower number of\\nimages and identities, we see this by comparing the CA-\\nSIA [44] and UMD [2] datasets; and (d) large number of\\nimages with smaller number of identities may not help, we\\nsee this from the VGG Face [24] dataset. Besides, we ana-\\nlyze the dataset uniformity or balance issue, i.e., number of\\nimages-per-identity, see bottom part of of Table 5. We use\\nthe MSCeleb [11] dataset for this experiment. We see that,\\nwhile maintaining certain balance is necessary, it is equal-\\nity important to train CNN with a larger dataset. We obtain\\nthe best performance by keeping only the identities with 30\\nimages or more.\\nNext, we analyze the results based on different CNN\\ncomponents and models. Table 6 and Fig. 2 present the\\nresults with different forms, where we train all settings with\\nthe CASIA [44] dataset. Our observations are: (a) the pro-\\nposed CNN model obtains better performance by including\\nfeature normalization (FN) before loss computation, we see\\nthis by comparing with the center loss [39] and without FN\\nbased results and (b) it obtains better accuracy than the other\\ncommonly used CNNs (for FR), such as the VGG-Net [24]\\nand CASIA-Net [44]. Note that, we do not directly com-\\npare with other loss functions (within our CNN model) as\\nthe center loss [39] has been shown to be more efﬁcient than\\nthose. Additionally, we trained our CNN with ReLU instead\\nof PReLU and observe that it decreases accuracy by approx-\\nimately 0.5%. In terms of complexity (measured with the\\nTable 5: Analysis of the inﬂuences from training databases, size and\\nnumber of classes. T@F denotes the True Accept Rate at a ﬁxed False\\nAccept Rate (TAR@FAR).\\nAspect\\nAdd. info\\nAcc\\n%\\nT@F\\n0.01\\nDB\\nSize, Class\\nCASIA [44]\\n0.43M, 10.6K\\n99.00\\n0.988\\nPose-CASIA [23]\\n1.26M, 10.6K\\n99.15\\n0.992\\nUMDFaces [2]\\n0.34M, 8.5K\\n99.15\\n0.992\\nVGG Face [24]\\n1.6M, 2.6K\\n98.40\\n0.975\\nMSCeleb [11]\\n4.2M, 62.5K\\n99.62\\n0.997\\nMin samp/id\\nSize, Class\\n10\\n4.48M, 62.7K\\n99.56\\n0.996\\n30\\n4.47M, 62.5K\\n99.62\\n0.997\\n50\\n3.91M, 47.3K\\n99.60\\n0.997\\n70\\n3.11M, 33K\\n99.55\\n0.996\\n100\\n1.5M, 12.7K\\n99.23\\n0.991\\nFalse positive rate\\n0\\n0.02\\n0.04\\n0.06\\n0.08\\n0.1\\n0.12\\nTrue positive rate\\n0.88\\n0.9\\n0.92\\n0.94\\n0.96\\n0.98\\n1\\nBaseCNN\\nCasia-Net\\nBaseCNN+CL-FN\\nBaseCNN-FN\\nVGGNet\\nFigure 2: Illustration of the ROC plots for different CNN models evalu-\\nates on the LFW [14] dataset.\\nnumber of parameters in Table 6), our model is more com-\\nplex than the simpler models (Cas-Net and CN-mod). How-\\never, it is much simpler than the VGG-Net [24]. Results\\nindicate that, while a simpler model may limit7 the FR per-\\nformance, a complex model is prone to overﬁtting. Perhaps\\nthis is the reason why the VGG-Net [24] requires additional\\nﬁne-tuning on the target datasets. The above analyses jus-\\ntify the efﬁciency of our proposed CNN model.\\nWe observe that, feature normalization (FN) before the\\nloss computation plays a signiﬁcant role in the performance.\\nIn order to gain further insights, we conduct experiments\\nand visualize the features of the MNIST digits in the 2D\\nspace. This is similar to the visualization recently shown\\nin [39] and hence we also provide a comparison with the\\ncenter loss (CL). The CNN is composed of 6 convolution,\\n2 pool and 1 FC (with 2 neurons for 2D visualization) lay-\\n7We train the CN-mod (see Table 6) with the MSCeleb dataset and\\nobserved that, compared to our proposed CNN model CN-mod provides\\nlower results and generalizes poorly.\\n\\nTable 6: Study the inﬂuences from CNN related issues. All CNN models\\nare trained with the CASIA [44] dataset. CL- center loss [39], FN- feature\\nnormalization. CN-mod modiﬁes the Cas-Net [44] by replacing Pool layer\\nwith a FC layer of 512 neurons.\\nSettings\\n# params\\nAcc\\n%\\nT@F\\n0.01\\nBase-CNN (proposed)\\n40.5M\\n99.00\\n0.988\\nBase-CNN - FN\\n40.5M\\n97.40\\n0.954\\nBase-CNN + CL - FN\\n44.8M\\n98.85\\n0.986\\nVGG-Net [24]\\n182M\\n95.15\\n0.883\\nCas-Net [44]\\n6M\\n97.10\\n0.938\\nCN-mod\\n8M\\n97.50\\n0.956\\ners. We optimize it using the softmax loss. Fig. 3 provides\\nthe illustration, from which we observe that: (a) FN pro-\\nvides a better feature discrimination in the normalized 2D\\nspace, see Fig. 3-b; (b) CL enforces the features towards its\\nrepresentative center and hence shows discrimination, see\\nFig. 3-c and (c) CL+FN does not provide much additional\\ndiscrimination, see Fig. 3-b and Fig. 3-d. These observa-\\ntions reveal that, by exploiting the FN appropriately we can\\nensure feature discrimination and hence no additional loss\\nfunction, e.g., CL, is necessary.\\na\\nb\\nc\\nd\\nFigure 3: 2D visualization of the MNIST [18] digits features, which are\\nobtained by using same baseline CNN model and training settings. CL\\n[39] parameters are set to λ = 0.003 and α = 0.5. a. CNN without FN\\nand CL; b. CNN with FN; c. CNN with CL; and d. CNN with FN and CL.\\nFinally, we investigate the incorrect results by observing\\nthe face image pairs in which DeepVisage failed. Appendix\\nA provides the illustrations of the false accept/reject cases\\nfrom the different datasets. We observe that, on LFW it\\nfailed (11/20 error cases) when the eyes are occluded by\\nglasses or a cap. Incorrect CACD results and higher false\\nrejection rate indicate that our method (although provides\\nbest accuracy) encounters difﬁculties to recognize the same\\nperson from the images of different ages. Incorrect results\\nfrom YTF often suffers from high pose and perhaps low im-\\nage resolution. IJB-A results reveal that our method needs\\nto take care of the face images with extreme pose varia-\\ntions. Indeed, during the IJB-A experiments, we are forced\\nto keep a large number of images as un-normalized due to\\nthe failure of landmarks detection for them. Based on em-\\npirical evidences, we believe that these un-normalized faces\\ncause the degradation of our performance. Besides, the re-\\nsults from YTF and IJB-A indicate that we may need to use\\na better distance computation strategy.\\n5. Conclusion\\nIn this paper we present a single-CNN based FR method\\nwhich achieves state-of-the-art performance and exhibits\\nexcellent ability of generalize across different FR datasets.\\nOur method, called DeepVisage, performs face veriﬁca-\\ntion based on a given pair of single images, templates and\\nvideos. It consists in a deep CNN model which is simple\\nand straightforward to train. Overall, DeepVisage is very\\neasy to implement, thanks to the residual learning frame-\\nwork, feature normalization, softmax loss and the simplest\\ndistance.\\nIt successfully demonstrates that, in order to\\nachieve state-of-the-art results it is not necessary to develop\\na complicated FR method by using complex training data\\npreparation and CNN learning procedure. We foresee sev-\\neral future perspectives of this work, such as: (a) train CNN\\nwith a larger and more balanced dataset, which can be con-\\nstructed by combining multiple publicly available datasets\\nor by adopting the face synthesizing strategy [23] with the\\nexisting one; (b) enhance FR performance by incorporating\\nfailure detection based technique [30], particularly for face\\nand landmarks detection and (c) incorporate better distance\\ncomputation method for the template and video compari-\\nson, e.g., use softmax based distance [23].\\nA. Analysis of the incorrect results\\nIn this section, we provide examples of the incorrect re-\\nsults observed from the face veriﬁcation experiments on dif-\\nferent datasets.\\nLFW [14]:\\nFigure 4 provides the examples of the failure\\ncases on the LFW [14] benchmark. The ratio of false accept\\nvs reject is 1:1.56. Note that our method achieves 99.62%\\naccuracy on LFW. In Figure 4(b) three pairs are marked\\nwith red colored rectangles. These pairs are erroneously la-\\nbeled in the dataset, which means our method makes correct\\njudgment on them and hence the accuracy further increases\\nto 99.67% by considering them as correct match.\\nCACD-VS [3]:\\nFigure 5 provides the examples of the fail-\\nure cases on the CACD [14] dataset. The ratio of false ac-\\ncept vs reject is 1:6.\\n\\n(a)\\n(b)\\nFigure 4: Illustration of the false accepted/rejected image\\npairs from the LFW [14] benchmark. (a) false accepted\\npairs and (b) false rejected pairs. The red colored rectan-\\ngles indicate the examples which were erroneously labeled\\nin the dataset.\\nYTF [40]:\\nFigure 6 provides few examples of the failure\\ncases on the YTF [40] dataset. The ratio of false accept vs\\nreject is 1:2.2. In Figure 6, we only show top three mistakes\\n(sorted based on their similarity score) in terms of false ac-\\ncept and reject.\\nIJB-A [16]:\\nFigure 7 provides few examples of the IJB-\\nA [16] failure cases. The ratio of false accept vs reject is\\n1:5.15. In Figure 7, we only show top three mistakes (sorted\\nbased on their similarity score). From the falsely rejected\\ntemplate pairs, we observe that: (a) one pair has only one\\nimage in the template; (b) the pre-processor fails to detect\\nface as well as landmarks and (c) the images in the tem-\\nplate have very high pose and large occlusion which causes\\nimportant face attributes to be absent.\\n(a)\\n(b)\\nFigure 5: Illustration of the false accepted/rejected image\\npairs from the CACD-VS [3] dataset. (a) false accepted\\npairs and (b) false rejected pairs.\\nReferences\\n[1] W. AbdAlmageed, Y. Wu, S. Rawls, S. Harel, T. Hassner,\\nI. Masi, J. Choi, J. Lekust, J. Kim, and P. Natarajan. Face\\n\\n(a)\\n(b)\\nFigure 6: Illustration of the false accepted/rejected video pairs from the YTF [40] dataset. (a) false accepted pairs and (b)\\nfalse rejected pairs.\\n(a)\\n(b)\\nFigure 7: Illustration of the false accepted/rejected template pairs from the IJB-A [16] dataset. (a) false accepted pairs and\\n(b) false rejected pairs.\\nrecognition using deep multi-pose representations. In IEEE\\nWACV, pages 1–9, 2016.\\n[2] A. Bansal, A. Nanduri, C. Castillo, R. Ranjan, and R. Chel-\\nlappa. Umdfaces: An annotated face dataset for training deep\\nnetworks. arXiv:1611.01484, 2016.\\n[3] B.-C. Chen, C.-S. Chen, and W. H. Hsu. Face recognition\\nand retrieval using cross-age reference coding with cross-age\\ncelebrity dataset. IEEE Trans. on Multimedia, 17(6):804–\\n815, 2015.\\n[4] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun. Bayesian\\n\\nface revisited: A joint formulation. In ECCV, pages 566–\\n579. 2012.\\n[5] J.-C. Chen, V. M. Patel, and R. Chellappa. Unconstrained\\nface veriﬁcation using deep cnn features.\\nIn 2016 IEEE\\nWACV, pages 1–9, 2016.\\n[6] J.-C. Chen, R. Ranjan, A. Kumar, C.-H. Chen, V. Patel, and\\nR. Chellappa. An end-to-end system for unconstrained face\\nveriﬁcation with deep convolutional neural networks.\\nIn\\nProc. of IEEE ICCV-W, pages 118–126, 2015.\\n[7] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity\\nmetric discriminatively, with application to face veriﬁcation.\\nIn Proc. of IEEE CVPR, pages 539–546, 2005.\\n[8] N. Crosswhite, J. Byrne, O. M. Parkhi, C. Stauffer, Q. Cao,\\nand A. Zisserman. Template adaptation for face veriﬁcation\\nand identiﬁcation. arXiv:1603.03958, 2016.\\n[9] C. Ding and D. Tao.\\nRobust face recognition via multi-\\nmodal deep face representation. IEEE Trans. on Multimedia,\\n17(11):2049–2058, 2015.\\n[10] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai,\\nT. Liu, X. Wang, and G. Wang. Recent advances in convolu-\\ntional neural networks. arXiv:1512.07108, 2015.\\n[11] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao. Ms-celeb-1m:\\nA dataset and benchmark for large-scale face recognition.\\nCoRR, abs/1607.08221, 2016.\\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into\\nrectiﬁers: Surpassing human-level performance on imagenet\\nclassiﬁcation. In Proc. of IEEE CVPR, pages 1026–1034,\\n2015.\\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\\nfor image recognition. In Proc. of IEEE CVPR, 2016.\\n[14] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller.\\nLabeled faces in the wild: A database for studying face\\nrecognition in unconstrained environments.\\nTechnical re-\\nport, Technical Report 07-49, University of Massachusetts,\\nAmherst, 2007.\\n[15] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\\ndeep network training by reducing internal covariate shift. In\\nProc. of ICML, pages 448–456, 2015.\\n[16] B. F. Klare, B. Klein, E. Taborsky, A. Blanton, J. Cheney,\\nK. Allen, P. Grother, A. Mah, M. Burge, and A. K. Jain.\\nPushing the frontiers of unconstrained face detection and\\nrecognition: Iarpa janus benchmark a.\\nIn Proc. of IEEE\\nCVPR, pages 1931–1939, 2015.\\n[17] E. Learned-Miller, G. Huang, A. RoyChowdhury, H. Li,\\nG. Hua, and G. B. Huang.\\nLabeled faces in the wild: A\\nsurvey. 2015.\\n[18] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\\nbased learning applied to document recognition. Proc. of the\\nIEEE, 86(11):2278–2324, 1998.\\n[19] S. Liao, Z. Lei, D. Yi, and S. Z. Li. A benchmark study of\\nlarge-scale unconstrained face recognition. In Proc. of IEEE\\nIJCB, pages 1–8, 2014.\\n[20] J. Liu, Y. Deng, and C. Huang. Targeting ultimate accuracy:\\nFace recognition via deep embedding. arXiv:1506.07310,\\n2015.\\n[21] X. Liu,\\nM. Kan,\\nW. Wu,\\nS. Shan,\\nand X. Chen.\\nVIPLFaceNet: An open source deep face recognition sdk.\\narXiv:1609.03892, 2016.\\n[22] I. Masi, S. Rawls, G. Medioni, and P. Natarajan. Pose-aware\\nface recognition in the wild. In Proc. of IEEE CVPR, pages\\n4838–4846, 2016.\\n[23] I. Masi, A. Tran, T. Hassner, J. T. Leksut, and G. Medioni.\\nDo We Really Need to Collect Millions of Faces for Effec-\\ntive Face Recognition? In ECCV, 2016.\\n[24] O. M. Parkhi, A. Vedaldi, and A. Zisserman.\\nDeep face\\nrecognition. Proc. of BMVC, 1(3):6, 2015.\\n[25] R. Ranjan, S. Sankaranarayanan, C. D. Castillo, and R. Chel-\\nlappa. An all-in-one convolutional neural network for face\\nanalysis. arXiv:1611.00851, 2016.\\n[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\\nRecognition Challenge. IJCV, 115(3):211–252, 2015.\\n[27] S. Sankaranarayanan, A. Alavi, C. Castillo, and R. Chel-\\nlappa. Triplet probabilistic embedding for face veriﬁcation\\nand clustering. arXiv:1604.05417, 2016.\\n[28] S.\\nSankaranarayanan,\\nA.\\nAlavi,\\nand\\nR.\\nChellappa.\\nTriplet\\nsimilarity\\nembedding\\nfor\\nface\\nveriﬁcation.\\narXiv:1602.03418, 2016.\\n[29] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-\\nﬁed embedding for face recognition and clustering. In Proc.\\nof IEEE CVPR, 2015.\\n[30] A. Steger, R. Timofte, and L. Van Gool. Failure detection for\\nfacial landmark detectors. arXiv:1608.06451, 2016.\\n[31] Y. Sun, D. Liang, X. Wang, and X. Tang.\\nDeepID3:\\nFace\\nrecognition\\nwith\\nvery\\ndeep\\nneural\\nnetworks.\\narXiv:1502.00873, 2015.\\n[32] Y. Sun, X. Wang, and X. Tang. Deeply learned face repre-\\nsentations are sparse, selective, and robust. In Proc. of IEEE\\nCVPR, pages 2892–2900, 2015.\\n[33] Y. Sun, X. Wang, and X. Tang. Sparsifying neural network\\nconnections for face recognition. In Proc. of IEEE CVPR,\\n2016.\\n[34] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:\\nClosing the gap to human-level performance in face veriﬁca-\\ntion. In Proc. of IEEE CVPR, pages 1701–1708, 2014.\\n[35] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Web-scale\\ntraining for face identiﬁcation.\\nIn Proc. of IEEE CVPR,\\npages 2746–2754, 2015.\\n[36] S. Theodoridis and K. Koutroumbas. Pattern Recognition,\\nFourth Edition. Academic Press, 4th edition, 2008.\\n[37] D. Wang, C. Otto, and A. K. Jain. Face search at scale. IEEE\\nTPAMI, 2016.\\n[38] Y. Wen, Z. Li, and Y. Qiao. Latent factor guided convolu-\\ntional neural networks for age-invariant face recognition. In\\nProc. of IEEE CVPR, pages 4893–4901, 2016.\\n[39] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-\\nture learning approach for deep face recognition. In Proc. of\\nECCV, pages 499–515. Springer, 2016.\\n[40] L. Wolf, T. Hassner, and I. Maoz. Face recognition in un-\\nconstrained videos with matched background similarity. In\\nProc. of IEEE CVPR, pages 529–534, 2011.\\n[41] X. Wu, R. He, Z. Sun, and T. Tan. A light cnn for deep face\\nrepresentation with noisy labels. arXiv:1511.02683, 2015.\\n\\n[42] J. Yang, P. Ren, D. Chen, F. Wen, H. Li, and G. Hua.\\nNeural aggregation network for video face recognition.\\narXiv:1603.05474, 2016.\\n[43] H. Ye, W. Shao, H. Wang, J. Ma, L. Wang, Y. Zheng, and\\nX. Xue. Face recognition via active annotation and learning.\\nIn Proc. of ACM Multimedia, pages 1058–1062. ACM, 2016.\\n[44] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face represen-\\ntation from scratch. arXiv:1411.7923, 2014.\\n[45] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detection\\nand alignment using multitask cascaded convolutional net-\\nworks. IEEE Signal Processing Letters, 23(10):1499–1503,\\nOct 2016.\\n[46] E. Zhou, Z. Cao, and Q. Yin.\\nNaive-deep face recog-\\nnition:\\nTouching the limit of lfw benchmark or not?\\narXiv:1501.04690, 2015.\\n',\n",
       "  'metadata': {'authors': ['Abul Hasnat',\n",
       "    'Julien Bohné',\n",
       "    'Jonathan Milgram',\n",
       "    'Stéphane Gentric',\n",
       "    'Liming Chen'],\n",
       "   'published': datetime.datetime(2017, 3, 24, 12, 41, 38, tzinfo=datetime.timezone.utc)}}]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from arxiv import Search, SortCriterion\n",
    "import fitz  # PyMuPDF\n",
    "from typing import List, Dict\n",
    "\n",
    "class PaperLoader:\n",
    "    def fetch_for_query(self, query: str) -> List[Dict]:\n",
    "        search = Search(\n",
    "            query=query,\n",
    "            max_results=3,\n",
    "            sort_by=SortCriterion.Relevance\n",
    "        )\n",
    "        return [self._process_result(r) for r in search.results()]\n",
    "    \n",
    "    def _process_result(self, result) -> Dict:\n",
    "        return {\n",
    "            \"title\": result.title,\n",
    "            \"text\": self._extract_text(result.pdf_url),\n",
    "            \"metadata\": {\n",
    "                \"authors\": [a.name for a in result.authors],\n",
    "                \"published\": result.published\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _extract_text(self, pdf_url: str) -> str:\n",
    "        import requests\n",
    "        response = requests.get(pdf_url)\n",
    "        with fitz.open(stream=response.content, filetype=\"pdf\") as doc:\n",
    "            return \"\\n\".join(page.get_text() for page in doc)\n",
    "        \n",
    "loader = PaperLoader()\n",
    "topic =\"loss function cnn\"\n",
    "\n",
    "\n",
    "papers = loader.fetch_for_query(topic)[:3]\n",
    "print(len(papers))\n",
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f7/jqgtrh7d1zd6rb0wwb3rzgd00000gn/T/ipykernel_92442/1400673255.py:36: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Literature review with MCP completed! Check literature_review_mcp.md\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Install if needed\n",
    "# pip install arxiv markdownify ollama\n",
    "# --------------------\n",
    "import arxiv\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "# Optional: if using Ollama, pip install ollama\n",
    "\n",
    "# -------------------------\n",
    "# Context Manager\n",
    "# -------------------------\n",
    "class ContextManager:\n",
    "    def __init__(self):\n",
    "        self.contexts = {}\n",
    "\n",
    "    def add(self, key, text):\n",
    "        self.contexts[key] = text\n",
    "\n",
    "    def get(self, key):\n",
    "        return self.contexts.get(key, \"\")\n",
    "\n",
    "    def get_full_context(self):\n",
    "        return \"\\n\\n\".join([f\"### {k}\\n{text}\" for k, text in self.contexts.items()])\n",
    "\n",
    "# -------------------------\n",
    "# Search and get abstract\n",
    "# -------------------------\n",
    "def search_papers(topic, max_results=1):\n",
    "    search = arxiv.Search(\n",
    "        query=topic,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "    )\n",
    "    results = []\n",
    "    for result in search.results():\n",
    "        results.append({\n",
    "            \"title\": result.title,\n",
    "            \"summary\": result.summary,\n",
    "            \"pdf_url\": result.pdf_url,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# Ollama helper\n",
    "# -------------------------\n",
    "def ask_model(prompt, model=\"llama3\"):\n",
    "    command = f'echo \"{prompt}\" | ollama run {model}'\n",
    "    result = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    return result.stdout.decode()\n",
    "\n",
    "# -------------------------\n",
    "# Chain-of-thought MCP steps\n",
    "# -------------------------\n",
    "def analyze_paper_with_mcp(paper):\n",
    "    context = ContextManager()\n",
    "\n",
    "    abstract = paper[\"summary\"]\n",
    "\n",
    "    # Step 1: Extract methods\n",
    "    prompt1 = f\"\"\"\n",
    "Given the following abstract:\n",
    "\n",
    "{abstract}\n",
    "\n",
    "Extract only the METHODS used (e.g., model architectures, algorithms, datasets, evaluation protocols).\n",
    "\"\"\"\n",
    "    methods = ask_model(prompt1)\n",
    "    context.add(\"Methods\", methods)\n",
    "\n",
    "    # Step 2: Extract results & conclusions\n",
    "    prompt2 = f\"\"\"\n",
    "Given the abstract and methods:\n",
    "\n",
    "Abstract: {abstract}\n",
    "\n",
    "Methods: {methods}\n",
    "\n",
    "Summarize the main RESULTS and CONCLUSIONS.\n",
    "\"\"\"\n",
    "    results = ask_model(prompt2)\n",
    "    context.add(\"Results\", results)\n",
    "\n",
    "    # Step 3: Analyze limitations and gaps\n",
    "    full_context = context.get_full_context()\n",
    "    prompt3 = f\"\"\"\n",
    "Based on the following context:\n",
    "\n",
    "{full_context}\n",
    "\n",
    "Analyze and list possible LIMITATIONS and potential GAPS or missing areas in this research.\n",
    "\"\"\"\n",
    "    gaps = ask_model(prompt3)\n",
    "    context.add(\"Gaps\", gaps)\n",
    "\n",
    "    # Step 4: Suggest new experiments\n",
    "    full_context = context.get_full_context()\n",
    "    prompt4 = f\"\"\"\n",
    "Based on all the above context:\n",
    "\n",
    "{full_context}\n",
    "\n",
    "Propose NEW EXPERIMENTS or RESEARCH DIRECTIONS that could build upon this work. Explain why they could be valuable.\n",
    "\"\"\"\n",
    "    suggestions = ask_model(prompt4)\n",
    "    context.add(\"Suggestions\", suggestions)\n",
    "\n",
    "    return context\n",
    "\n",
    "# -------------------------\n",
    "# Save to markdown\n",
    "# -------------------------\n",
    "def save_markdown(context, title):\n",
    "    md_text = md(context.get_full_context())\n",
    "    with open(\"literature_review_mcp.md\", \"w\") as f:\n",
    "        f.write(f\"# {title}\\n\\n\")\n",
    "        f.write(md_text)\n",
    "\n",
    "# -------------------------\n",
    "# Main run\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    topic = \"Diffusion models for medical image segmentation\"\n",
    "    papers = search_papers(topic, max_results=1)\n",
    "\n",
    "    if papers:\n",
    "        paper = papers[0]\n",
    "        context = analyze_paper_with_mcp(paper)\n",
    "        save_markdown(context, paper[\"title\"])\n",
    "        print(\"✅ Literature review with MCP completed! Check literature_review_mcp.md\")\n",
    "    else:\n",
    "        print(\"❌ No papers found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Search Queries:\n",
      "- [\"loss functions in convolutional neural networks\",\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f7/jqgtrh7d1zd6rb0wwb3rzgd00000gn/T/ipykernel_92442/3425738652.py:12: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  return [self._process_result(r) for r in search.results()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "- \"optimization metrics for cnn models\",\n",
      "[]\n",
      "- \"performance evaluation criteria for deep learning architectures\",\n",
      "[]\n",
      "- \"convolutional neural network loss functions\",\n",
      "[]\n",
      "- \"cnn model evaluation metrics\"]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def rewrite_query_with_llm(user_query):\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant for academic search.\n",
    "    ==============================================\n",
    "    Rewrite the following query into 3–5 concise academic search strings\n",
    "    optimized for finding relevant research papers in scientific databases.\n",
    "    Expand acronyms, include common synonyms and dont include any bollean expressions like (AND, OR).\n",
    "    ==============================================\n",
    "    Query: {user_query}\n",
    "    ==============================================\n",
    "    Output should be an array of strings and nothing else.\n",
    "    \"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", \"llama3.1:8b-instruct-q5_K_M\"],\n",
    "        input=prompt.encode(),\n",
    "        capture_output=True\n",
    "    )\n",
    "    output = result.stdout.decode().strip()\n",
    "    queries = [q.strip() for q in output.split(\"\\n\") if q.strip()]\n",
    "    return queries\n",
    "\n",
    "# Example usage\n",
    "queries = rewrite_query_with_llm(\"What are the loss functions used in CNN modelling?\")\n",
    "print(\"Generated Search Queries:\")\n",
    "\n",
    "loader = PaperLoader()\n",
    "\n",
    "for q in queries:\n",
    "    print(\"-\", q)\n",
    "\n",
    "\n",
    "    papers = loader.fetch_for_query(q)\n",
    "    print(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Title: Face r-cnn\n",
      "Authors: ['H Wang', 'Z Li', 'X Ji', 'Y Wang']\n",
      "Year: 2017\n",
      "URL: https://arxiv.org/abs/1706.01061\n",
      "Abstract: property of the face detection task, we improve the Faster R-CNN framework in  loss function  called center loss [33], we design a new multi-task loss function in the Fast R-CNN model to\n",
      "\n",
      "Full Text: Face R-CNN\n",
      "Hao Wang\n",
      "Zhifeng Li∗Xing Ji\n",
      "Yitong Wang\n",
      "Tencent AI Lab, China\n",
      "{hawelwang,michaelzfli,deni\n",
      "Title: Loss function search for face recognition\n",
      "Authors: ['X Wang', 'S Wang', 'C Chi', 'S Zhang']\n",
      "Year: 2020\n",
      "URL: https://proceedings.mlr.press/v119/wang20t.html\n",
      "Abstract: target of our loss function search is to maximize the model Mw’ the model Mw is obtained by  minimizing the following search  the performance of face identification and the ROC curves to\n",
      "\n",
      "Full Text: Loss Function Search for Face Recognition\n",
      "Xiaobo Wang * 1 Shuo Wang * 1 Cheng Chi 2 Shifeng Zhang 2 \n",
      "Title: Person re-identification by multi-channel parts-based cnn with improved triplet loss function\n",
      "Authors: ['D Cheng', 'Y Gong', 'S Zhou', 'J Wang']\n",
      "Year: 2016\n",
      "URL: http://openaccess.thecvf.com/content_cvpr_2016/html/Cheng_Person_Re-Identification_by_CVPR_2016_paper.html\n",
      "Abstract: method, then elaborate the network architecture of the proposed multi-channel CNN model.  Finally, we present the improved triplet loss function used to train the proposed CNN model.\n",
      "\n",
      "Full Text: Person Re-Identiﬁcation by Multi-Channel Parts-Based CNN with Improved\n",
      "Triplet Loss Function\n",
      "De Chen\n",
      "Title: Wing loss for robust facial landmark localisation with convolutional neural networks\n",
      "Authors: ['ZH Feng', 'J Kittler', 'M Awais']\n",
      "Year: 2018\n",
      "URL: http://openaccess.thecvf.com/content_cvpr_2018/html/Feng_Wing_Loss_for_CVPR_2018_paper.html\n",
      "Abstract: new loss function, namely Wing loss, for robust facial landmark  -based localisation model,  more attention should be paid to  face detection and in-plane head rotations. Then the second\n",
      "\n",
      "Full Text: Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural\n",
      "Networks\n",
      "Zhen-Hua Feng1 \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "from scholarly import scholarly\n",
    "from arxiv import Search, SortCriterion\n",
    "from typing import List, Dict\n",
    "\n",
    "class MultiSourcePaperLoader:\n",
    "    def __init__(self, semantic_api_key: str = None):\n",
    "        self.semantic_base_url = \"https://api.semanticscholar.org/graph/v1\"\n",
    "        self.semantic_api_key = semantic_api_key\n",
    "\n",
    "    from urllib.parse import urlparse\n",
    "\n",
    "    def get_pdf_url(self, paper_url: str) -> str:\n",
    "        \"\"\"\n",
    "        Convert a paper HTML page URL to a direct PDF URL for known publishers.\n",
    "        Supports:\n",
    "        - PMLR Proceedings (proceedings.mlr.press)\n",
    "        - arXiv (arxiv.org)\n",
    "        - CVF Open Access (openaccess.thecvf.com)\n",
    "        \"\"\"\n",
    "        if \"proceedings.mlr.press\" in paper_url:\n",
    "            # Extract the volume and paper_id\n",
    "            parts = paper_url.rstrip(\"/\").split(\"/\")\n",
    "            volume = parts[-2]   # e.g., v119\n",
    "            paper_id_html = parts[-1]  # e.g., wang20t.html\n",
    "            paper_id = paper_id_html.replace(\".html\", \"\")\n",
    "            return f\"https://proceedings.mlr.press/{volume}/{paper_id}/{paper_id}.pdf\"\n",
    "        \n",
    "        elif \"arxiv.org/abs/\" in paper_url:\n",
    "            # Example: https://arxiv.org/abs/1706.01061 -> /pdf/1706.01061.pdf\n",
    "            return paper_url.replace(\"abs\", \"pdf\") + \".pdf\"\n",
    "        \n",
    "        elif \"openaccess.thecvf.com\" in paper_url:\n",
    "            # Example:\n",
    "            # https://openaccess.thecvf.com/content_cvpr_2016/html/Cheng_...html\n",
    "            # -> https://openaccess.thecvf.com/content_cvpr_2016/papers/Cheng_...pdf\n",
    "            return paper_url.replace(\".html\", \".pdf\").replace(\"/html/\", \"/papers/\")\n",
    "        \n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # ---------------- SEMANTIC SCHOLAR ----------------\n",
    "    def fetch_from_semantic_scholar(self, query: str, max_results=3) -> List[Dict]:\n",
    "        headers = {}\n",
    "        if self.semantic_api_key:\n",
    "            headers[\"x-api-key\"] = self.semantic_api_key\n",
    "        \n",
    "        url = f\"{self.semantic_base_url}/paper/search\"\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"limit\": max_results,\n",
    "            \"fields\": \"title,url,abstract,authors,year,openAccessPdf\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            res = requests.get(url, headers=headers, params=params, timeout=15)\n",
    "            res.raise_for_status()\n",
    "            data = res.json()\n",
    "            results = []\n",
    "            for paper in data.get(\"data\", []):\n",
    "                pdf_url = paper.get(\"openAccessPdf\", {}).get(\"url\")\n",
    "                results.append({\n",
    "                    \"title\": paper[\"title\"],\n",
    "                    \"abstract\": paper.get(\"abstract\"),\n",
    "                    \"authors\": [a[\"name\"] for a in paper.get(\"authors\", [])],\n",
    "                    \"year\": paper.get(\"year\"),\n",
    "                    \"url\": paper.get(\"url\"),\n",
    "                    \"pdf_url\": pdf_url\n",
    "                })\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"[Semantic Scholar Error] {e}\")\n",
    "            return []\n",
    "\n",
    "    # ---------------- ARXIV ----------------\n",
    "    def fetch_from_arxiv(self, query: str, max_results=3) -> List[Dict]:\n",
    "        try:\n",
    "            search = Search(query=query, max_results=max_results, sort_by=SortCriterion.Relevance)\n",
    "            results = []\n",
    "            for r in search.results():\n",
    "                results.append({\n",
    "                    \"title\": r.title,\n",
    "                    \"abstract\": r.summary,\n",
    "                    \"authors\": [a.name for a in r.authors],\n",
    "                    \"year\": r.published.year,\n",
    "                    \"url\": r.entry_id,\n",
    "                    \"pdf_url\": r.pdf_url\n",
    "                })\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"[arXiv Error] {e}\")\n",
    "            return []\n",
    "\n",
    "    # ---------------- GOOGLE SCHOLAR ----------------\n",
    "    def fetch_from_google_scholar(self, query: str, max_results=3) -> List[Dict]:\n",
    "        try:\n",
    "            search_query = scholarly.search_pubs(query)\n",
    "            results = []\n",
    "            for i, paper in enumerate(search_query):\n",
    "                if i >= max_results:\n",
    "                    break\n",
    "\n",
    "                pdf_url = self.get_pdf_url(paper.get(\"pub_url\", \"\"))\n",
    "                if not pdf_url:  # Skip if PDF URL can't be determined\n",
    "                    continue\n",
    "                results.append({\n",
    "                    \"title\": paper.get(\"bib\", {}).get(\"title\"),\n",
    "                    \"abstract\": paper.get(\"bib\", {}).get(\"abstract\"),\n",
    "                    \"authors\": paper.get(\"bib\", {}).get(\"author\"),\n",
    "                    \"year\": paper.get(\"bib\", {}).get(\"pub_year\"),\n",
    "                    \"url\": paper.get(\"pub_url\"),\n",
    "                    \"pdf_url\": pdf_url  # Needs manual check\n",
    "                })\n",
    "\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"[Google Scholar Error] {e}\")\n",
    "            return []\n",
    "\n",
    "    # ---------------- PDF TEXT EXTRACTION ----------------\n",
    "    def _extract_text_from_pdf(self, pdf_url: str) -> str:\n",
    "        try:\n",
    "            res = requests.get(pdf_url, timeout=20)\n",
    "            res.raise_for_status()\n",
    "            with fitz.open(stream=res.content, filetype=\"pdf\") as doc:\n",
    "                return \"\\n\".join(page.get_text() for page in doc)\n",
    "        except Exception as e:\n",
    "            print(f\"[PDF Extraction Error] {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    # ---------------- MASTER FETCH METHOD ----------------\n",
    "    def fetch_papers(self, query: str, max_results=3) -> List[Dict]:\n",
    "        results = []\n",
    "\n",
    "        # Try Semantic Scholar\n",
    "        results.extend(self.fetch_from_semantic_scholar(query, max_results))\n",
    "        # If insufficient, try arXiv\n",
    "        # print(len(results))\n",
    "        if len(results) < max_results:\n",
    "            results.extend(self.fetch_from_arxiv(query, max_results))\n",
    "        # print(len(results))\n",
    "        \n",
    "        # If still insufficient, try Google Scholar\n",
    "        if len(results) < max_results:\n",
    "            results.extend(self.fetch_from_google_scholar(query, max_results))\n",
    "\n",
    "\n",
    "        # Remove duplicates based on title\n",
    "        seen_titles = set()\n",
    "        unique_results = []\n",
    "        for paper in results:\n",
    "            if paper[\"title\"] and paper[\"title\"].lower() not in seen_titles:\n",
    "                seen_titles.add(paper[\"title\"].lower())\n",
    "                unique_results.append(paper)\n",
    "\n",
    "        # Fetch PDF text if available\n",
    "        for paper in unique_results:\n",
    "            if paper.get(\"pdf_url\"):\n",
    "                paper[\"full_text\"] = self._extract_text_from_pdf(paper[\"pdf_url\"])\n",
    "            else:\n",
    "                paper[\"full_text\"] = \"\"\n",
    "\n",
    "        return unique_results\n",
    "\n",
    "# ---------------- Example usage ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    loader = MultiSourcePaperLoader(semantic_api_key=None)  # Add key if available\n",
    "    papers = loader.fetch_papers(\"What are the loss functions used in CNN modelling for face detection?\", max_results=10)\n",
    "\n",
    "    for p in papers:\n",
    "        print(f\"Title: {p['title']}\")\n",
    "        print(f\"Authors: {p['authors']}\")\n",
    "        print(f\"Year: {p['year']}\")\n",
    "        print(f\"URL: {p['url']}\")\n",
    "        print(f\"Abstract: {p['abstract'][:200] if p['abstract'] else 'N/A'}\\n\")\n",
    "        print(f\"Full Text: {p['full_text'][:100]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF download failed: 404 Client Error: Not Found for url: https://arxiv.org/pdf/Innovative%20deep%20learning%20architectures%20for%20medical%20image%20diagnosis:%20a%20comprehensive%20review%20of%20convolutional,%20recurrent,%20and%20transformer%20models\n",
      "Title: Innovative deep learning architectures for medical image diagnosis: a comprehensive review of convolutional, recurrent, and transformer models\n",
      "Text length: 180 chars\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
